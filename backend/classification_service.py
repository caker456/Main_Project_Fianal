"""
ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ ì„œë¹„ìŠ¤
í•™ìŠµëœ 2-Task BERT ëª¨ë¸ ì‚¬ìš© (ê¸°ê´€, ë¬¸ì„œìœ í˜•)
"""
import os
import json
import gc
import torch
import torch.nn as nn
import numpy as np
from typing import Dict

try:
    from transformers import AutoTokenizer, AutoConfig, AutoModel

    # 2-Task BERT ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
    class TwoTaskBertModel(nn.Module):
        """2ê°œì˜ ë¶„ë¥˜ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” BERT ëª¨ë¸"""

        def __init__(self, config, model_name, num_labels_dict):
            super().__init__()
            self.bert = AutoModel.from_pretrained(model_name, config=config)

            # ê° íƒœìŠ¤í¬ë³„ ë¶„ë¥˜ í—¤ë“œ
            self.classifiers = nn.ModuleDict({
                task: nn.Linear(config.hidden_size, num_labels)
                for task, num_labels in num_labels_dict.items()
            })

        def forward(self, input_ids, attention_mask=None, token_type_ids=None, **kwargs):
            # BERT ì¸ì½”ë”©
            outputs = self.bert(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids
            )

            # [CLS] í† í°ì˜ ì¶œë ¥ ì‚¬ìš©
            pooled_output = outputs.last_hidden_state[:, 0, :]

            # ê° íƒœìŠ¤í¬ë³„ ë¡œì§“ ê³„ì‚°
            logits = {
                task: classifier(pooled_output)
                for task, classifier in self.classifiers.items()
            }

            # transformers ìŠ¤íƒ€ì¼ ì¶œë ¥
            class ModelOutput:
                def __init__(self, logits):
                    self.logits = logits

            return ModelOutput(logits)

    BERT_AVAILABLE = True
except Exception as e:
    BERT_AVAILABLE = False
    TwoTaskBertModel = None
    print(f"âš ï¸ BERT model not available: {e}")


class ClassificationService:
    """ë¬¸ì„œ ë¶„ë¥˜ ì„œë¹„ìŠ¤ - Lazy loadingìœ¼ë¡œ VRAM íš¨ìœ¨ì  ì‚¬ìš©"""

    def __init__(self, model_dir: str = None):
        """
        Args:
            model_dir: í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
        """
        if model_dir is None:
            model_dir = os.path.join(os.path.dirname(__file__), "twotask_bert_model")

        self.model_dir = model_dir
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        self.label_mappings = None
        self._is_loaded = False

    def __enter__(self):
        """Context manager ì§„ì…"""
        self._ensure_loaded()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ - ë©”ëª¨ë¦¬ í•´ì œ"""
        self.cleanup()
        return False

    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            # ë ˆì´ë¸” ë§¤í•‘ ë¡œë“œ
            label_mapping_path = os.path.join(self.model_dir, "label_mappings.json")
            with open(label_mapping_path, 'r', encoding='utf-8') as f:
                self.label_mappings = json.load(f)

            # Config ë¡œë“œ
            config_path = os.path.join(self.model_dir, "config.json")
            with open(config_path, 'r', encoding='utf-8') as f:
                model_config = json.load(f)

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            print(f"Loading tokenizer from {self.model_dir}...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_dir)

            # ëª¨ë¸ ë¡œë“œ
            print(f"Loading 2-Task model from {self.model_dir}...")
            config = AutoConfig.from_pretrained(model_config['model_name'])
            self.model = TwoTaskBertModel(
                config=config,
                model_name=model_config['model_name'],
                num_labels_dict=model_config['num_labels']
            )

            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
            model_path = os.path.join(self.model_dir, "model.pt")
            self.model.load_state_dict(torch.load(model_path, map_location=self.device))

            self.model.to(self.device)
            self.model.eval()

            print(f"âœ“ 2-Task Model loaded successfully on {self.device}")
            print(f"  Tasks: ê¸°ê´€ ({self.label_mappings['ê¸°ê´€']['num_labels']} labels), "
                  f"ë¬¸ì„œìœ í˜• ({self.label_mappings['ë¬¸ì„œìœ í˜•']['num_labels']} labels)")

        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            self.model = None

    def _ensure_loaded(self):
        """ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¡œë“œ"""
        if not self._is_loaded and BERT_AVAILABLE and TwoTaskBertModel:
            self._load_model()
            if self.model is not None:
                self._is_loaded = True

    def cleanup(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ í•´ì œ"""
        if self._is_loaded:
            print(f"ğŸ§¹ Cleaning up BERT model from {self.device}...")

            # ëª¨ë¸ ì‚­ì œ
            if self.model is not None:
                del self.model
                self.model = None

            if self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None

            # GPU ë©”ëª¨ë¦¬ í•´ì œ
            if self.device == "cuda":
                torch.cuda.empty_cache()

            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()

            self._is_loaded = False
            print(f"âœ“ BERT model cleaned up")

    def predict(self, text: str, return_probs: bool = False) -> Dict:
        """
        í…ìŠ¤íŠ¸ì— ëŒ€í•œ 2ê°œ ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            return_probs: í™•ë¥ ê°’ë„ ë°˜í™˜í• ì§€ ì—¬ë¶€

        Returns:
            {
                "ê¸°ê´€": "ì˜ì‚¬êµ­ ì˜ì•ˆê³¼",
                "ë¬¸ì„œìœ í˜•": "ì˜ì•ˆì›ë¬¸",
                "confidence": {
                    "ê¸°ê´€": 0.95,
                    "ë¬¸ì„œìœ í˜•": 0.98
                },
                "probabilities": {  # return_probs=Trueì¸ ê²½ìš°
                    "ê¸°ê´€": {"ì˜ì‚¬êµ­ ì˜ì•ˆê³¼": 0.95, ...},
                    "ë¬¸ì„œìœ í˜•": {"ì˜ì•ˆì›ë¬¸": 0.98, ...}
                }
            }
        """
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¡œë“œ
        self._ensure_loaded()

        if not BERT_AVAILABLE or self.model is None:
            return {
                "ê¸°ê´€": "Unknown",
                "ë¬¸ì„œìœ í˜•": "Unknown",
                "confidence": {"ê¸°ê´€": 0.0, "ë¬¸ì„œìœ í˜•": 0.0},
                "error": "Model not available"
            }

        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )

        # GPUë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits

        # ê²°ê³¼ íŒŒì‹±
        result = {"confidence": {}}

        for task_name in ['ê¸°ê´€', 'ë¬¸ì„œìœ í˜•']:
            task_logits = logits[task_name].cpu().numpy()[0]

            # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë ˆì´ë¸”
            pred_id = int(np.argmax(task_logits))
            pred_label = self.label_mappings[task_name]['id2label'][str(pred_id)]

            result[task_name] = pred_label

            # í™•ë¥ ê°’ ê³„ì‚°
            probs = torch.softmax(torch.tensor(task_logits), dim=0).numpy()
            result["confidence"][task_name] = float(probs[pred_id])

            if return_probs:
                # ìƒìœ„ 5ê°œ ë ˆì´ë¸”ê³¼ í™•ë¥ 
                top_k = min(5, len(probs))
                top_indices = np.argsort(probs)[-top_k:][::-1]

                if 'probabilities' not in result:
                    result['probabilities'] = {}

                result['probabilities'][task_name] = {
                    self.label_mappings[task_name]['id2label'][str(idx)]: float(probs[idx])
                    for idx in top_indices
                }

        return result

    def predict_batch(self, texts: list, return_probs: bool = False) -> list:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë°°ì¹˜ ì˜ˆì¸¡"""
        self._ensure_loaded()
        results = []
        for text in texts:
            result = self.predict(text, return_probs=return_probs)
            results.append(result)
        return results


# í—¬í¼ í•¨ìˆ˜ - ê°„í¸í•œ ì‚¬ìš©ì„ ìœ„í•œ ë˜í¼
def get_classification_service():
    """ClassificationService ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜ (context manager ì‚¬ìš© ê¶Œì¥)"""
    return ClassificationService()
