
import shutil, os, json, time
from typing import Optional
from fastapi import APIRouter, UploadFile, File, Query,Form,HTTPException, Request, Response
from fastapi.responses import JSONResponse
from zip_utiles import extract_zip
from pydantic import BaseModel
from datetime import datetime
from login import login_member, get_current_user, logout_member
from member import add_member, update_member, delete_member, get_member_by_id, get_total_member_count
from db_conn import db_pool
from uploads import upload_files
from tempfile import NamedTemporaryFile
from PyPDF2 import PdfReader

# OCR ë° ë¶„ë¥˜ ì„œë¹„ìŠ¤
try:
    from ocr_service import get_ocr_service, OCR_AVAILABLE
except Exception as e:
    OCR_AVAILABLE = False
    print(f"âš ï¸ OCR service not available: {e}")

try:
    from classification_service import get_classification_service
    CLASSIFICATION_AVAILABLE = True
except Exception as e:
    CLASSIFICATION_AVAILABLE = False
    print(f"âš ï¸ Classification service not available: {e}")
        # router.py




router = APIRouter(prefix="/api")

# ì¸ì¦ ê´€ë ¨ ë¼ìš°í„° (prefix ì—†ìŒ)
auth_router = APIRouter()


# ============================================================
# ì‘ì—… ì´ë ¥ ë¡œê·¸ í—¬í¼ í•¨ìˆ˜
# ============================================================

def log_processing(conn, doc_id: int, filename: str, process_type: str, status: str, message: str):
    """
    ì‘ì—… ì´ë ¥ ë¡œê·¸ ê¸°ë¡

    Args:
        conn: DB ì—°ê²°
        doc_id: ë¬¸ì„œ ID
        filename: íŒŒì¼ëª…
        process_type: 'OCR', 'CLASSIFICATION', 'UPLOAD'
        status: 'SUCCESS', 'FAILED'
        message: ë¡œê·¸ ë©”ì‹œì§€
    """
    try:
        cur = conn.cursor()

        # processing_log í…Œì´ë¸” ìƒì„± (ì—†ìœ¼ë©´)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS processing_log (
                log_id SERIAL PRIMARY KEY,
                doc_id INTEGER,
                process_type VARCHAR(50) NOT NULL,
                status VARCHAR(50) NOT NULL,
                message TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (doc_id) REFERENCES pdf_documents(doc_id) ON DELETE CASCADE
            )
        """)

        # filename ì»¬ëŸ¼ ì¶”ê°€ (ì—†ìœ¼ë©´)
        cur.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name='processing_log' AND column_name='filename'
        """)
        if not cur.fetchone():
            cur.execute("ALTER TABLE processing_log ADD COLUMN filename VARCHAR(500)")
            print("âœ… processing_log í…Œì´ë¸”ì— filename ì»¬ëŸ¼ ì¶”ê°€")

        # ì¸ë±ìŠ¤ ìƒì„± (ì—†ìœ¼ë©´)
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_processing_log_created_at
            ON processing_log(created_at DESC)
        """)

        # ë¡œê·¸ ê¸°ë¡
        cur.execute("""
            INSERT INTO processing_log
            (doc_id, filename, process_type, status, message)
            VALUES (%s, %s, %s, %s, %s)
        """, (doc_id, filename, process_type, status, message))

        conn.commit()
        print(f"âœ… ì‘ì—… ë¡œê·¸ ê¸°ë¡: [{process_type}] {message}")

    except Exception as e:
        print(f"âš ï¸ ì‘ì—… ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        conn.rollback()


@router.post("/upload")
async def upload_res(request: Request, file: UploadFile = File(...), folder_path: str = Form(None)):
    try:
        return await upload_files(request, file, folder_path)

    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@router.post("/folders/create")
async def create_folder(request: Request, folder_name: str = Form(...)):
    """ìƒˆ í´ë” ìƒì„±"""
    conn = db_pool.get_conn()
    cur = None

    try:
        if not request.session.get("user"):
            raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")

        userid = request.session["user"].get("member_id")
        cur = conn.cursor()

        # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cur.execute("""
            SELECT member_id, id, name
            FROM member_info
            WHERE member_id = %s
        """, (userid,))
        member_row = cur.fetchone()

        if not member_row:
            raise HTTPException(status_code=404, detail="ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        member_id, user_uid, username = member_row

        # í´ë” ê²½ë¡œ ìƒì„± (ì‹¤ì œ íŒŒì¼ì‹œìŠ¤í…œ)
        folder_path = os.path.join(".", "upload", username, user_uid, folder_name).replace("\\", "/")
        os.makedirs(folder_path, exist_ok=True)

        # DBì— í´ë” ì •ë³´ ì €ì¥ (ë¹ˆ í´ë”ë„ í‘œì‹œí•˜ê¸° ìœ„í•´)
        try:
            cur.execute("""
                INSERT INTO folders (member_id, folder_name, folder_path, created_at)
                VALUES (%s, %s, %s, %s)
            """, (member_id, folder_name, folder_path, datetime.now()))
            conn.commit()
        except Exception as db_err:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í´ë”ì¼ ê²½ìš° ë¬´ì‹œ
            conn.rollback()
            print(f"í´ë” DB ì €ì¥ ì‹¤íŒ¨ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {db_err}")

        return {
            "success": True,
            "message": f"í´ë” '{folder_name}'ì´(ê°€) ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤",
            "folder_name": folder_name,
            "folder_path": f"{username}/{user_uid}/{folder_name}"
        }

    except HTTPException as he:
        raise he
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"í´ë” ìƒì„± ì‹¤íŒ¨: {str(e)}")
    finally:
        if cur:
            cur.close()
        db_pool.release_conn(conn)


@router.post("/folders/upload")
async def upload_folder(request: Request, files: list[UploadFile] = File(...)):
    """í´ë” ì „ì²´ ì—…ë¡œë“œ (PDF íŒŒì¼ë§Œ ì¶”ì¶œ)"""
    conn = db_pool.get_conn()
    cur = None

    try:
        if not request.session.get("user"):
            raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")

        userid = request.session["user"].get("member_id")
        cur = conn.cursor()

        # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cur.execute("""
            SELECT member_id, id, name
            FROM member_info
            WHERE member_id = %s
        """, (userid,))
        member_row = cur.fetchone()

        if not member_row:
            raise HTTPException(status_code=404, detail="ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        member_id, user_uid, username = member_row

        uploaded_files = []
        skipped_files = []
        created_folders = set()  # ìƒì„±ëœ í´ë” ëª©ë¡

        for file in files:
            # PDF íŒŒì¼ë§Œ ì²˜ë¦¬
            if not file.filename.lower().endswith('.pdf'):
                skipped_files.append(file.filename)
                continue

            try:
                # íŒŒì¼ ê²½ë¡œ ìƒì„± (í´ë” êµ¬ì¡° ìœ ì§€)
                file_path = os.path.join(".", "upload", username, user_uid, file.filename).replace("\\", "/")
                folder_path = os.path.dirname(file_path)

                os.makedirs(folder_path, exist_ok=True)

                # í´ë” ì •ë³´ ìˆ˜ì§‘ (ì¤‘ê°„ í´ë”ë“¤ í¬í•¨)
                # ì˜ˆ: test/folder1/file.pdf -> test, test/folder1
                relative_path = file.filename
                path_parts = relative_path.split("/")
                for i in range(len(path_parts) - 1):  # ë§ˆì§€ë§‰ì€ íŒŒì¼ì´ë¯€ë¡œ ì œì™¸
                    folder_relative = "/".join(path_parts[:i+1])
                    folder_full = os.path.join(".", "upload", username, user_uid, folder_relative).replace("\\", "/")
                    created_folders.add((folder_relative, folder_full))

                # íŒŒì¼ ì €ì¥
                with open(file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)

                # PDF ì •ë³´ ì¶”ì¶œ
                page_count = len(PdfReader(file_path).pages)
                size = round(os.path.getsize(file_path) / (1024 * 1024), 3)

                # DBì— ì €ì¥
                cur.execute("""
                    INSERT INTO pdf_documents (member_id, filename, updated_at, status, page_count, file_size, upload_date)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)
                """, (member_id, file_path, datetime.now(), "upload", page_count, size, datetime.now()))

                uploaded_files.append(file.filename)

            except Exception as e:
                print(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {file.filename} - {str(e)}")
                skipped_files.append(file.filename)

        # í´ë” ì •ë³´ë¥¼ folders í…Œì´ë¸”ì— ì €ì¥
        for folder_name, folder_full_path in created_folders:
            try:
                cur.execute("""
                    INSERT INTO folders (member_id, folder_name, folder_path, created_at)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (member_id, folder_path) DO NOTHING
                """, (member_id, folder_name.split("/")[-1], folder_full_path, datetime.now()))
            except Exception as e:
                print(f"í´ë” DB ì €ì¥ ì‹¤íŒ¨: {folder_name} - {str(e)}")

        conn.commit()

        return {
            "success": True,
            "message": f"{len(uploaded_files)}ê°œì˜ PDF íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤",
            "uploaded_files": uploaded_files,
            "skipped_files": skipped_files
        }

    except HTTPException as he:
        if conn:
            conn.rollback()
        raise he
    except Exception as e:
        if conn:
            conn.rollback()
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"í´ë” ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    finally:
        if cur:
            cur.close()
        db_pool.release_conn(conn)














@router.delete("/remove")
def remove_file(path: str = Query(..., description="ì‚­ì œí•  íŒŒì¼ ê²½ë¡œ")):
    conn = db_pool.get_conn()
    cur = None

    try:
        cur = conn.cursor()

        # íŒŒì¼ ì •ë³´ ì¡°íšŒ (ë³€ê²½ì´ë ¥ ì €ì¥ìš©)
        cur.execute("SELECT doc_id, filename FROM pdf_documents WHERE filename = %s", (path,))
        file_info = cur.fetchone()

        if not file_info:
            return JSONResponse(status_code=404, content={"success": False, "message": "í•´ë‹¹ ê²½ë¡œì˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."})

        doc_id, filename = file_info
        file_name = filename.split('/')[-1]

        # ë³€ê²½ì´ë ¥ì— ì‚­ì œ ê¸°ë¡ ì¶”ê°€
        try:
            cur.execute("""
                INSERT INTO classification_history
                (doc_id, file_name, full_path, original_folder, change_type)
                VALUES (%s, %s, %s, %s, 'deleted')
            """, (doc_id, file_name, filename, filename))
        except Exception as history_error:
            print(f"âš ï¸  ë³€ê²½ì´ë ¥ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {history_error}")

        # ì‚­ì œ ì‹¤í–‰
        cur.execute("DELETE FROM pdf_documents WHERE filename = %s", (path,))
        conn.commit()

        return {"success": True, "message": f"{path} ì‚­ì œ ì™„ë£Œ"}

    except Exception as e:
        if conn:
            conn.rollback()
        return JSONResponse(status_code=500, content={"success": False, "error": str(e)})

    finally:
        if cur:
            cur.close()
        db_pool.release_conn(conn)


@router.delete("/folders/delete")
async def delete_folder(request: Request, folder_name: str = Query(...)):
    """í´ë” ì‚­ì œ (í´ë” ë‚´ ëª¨ë“  íŒŒì¼ë„ ì‚­ì œ)"""
    conn = db_pool.get_conn()
    cur = None

    try:
        if not request.session.get("user"):
            raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")

        userid = request.session["user"].get("member_id")
        cur = conn.cursor()

        # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cur.execute("""
            SELECT member_id, id, name
            FROM member_info
            WHERE member_id = %s
        """, (userid,))
        member_row = cur.fetchone()

        if not member_row:
            raise HTTPException(status_code=404, detail="ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        member_id, user_uid, username = member_row

        # í´ë” ê²½ë¡œ ìƒì„±
        folder_path = os.path.join(".", "upload", username, user_uid, folder_name).replace("\\", "/")

        # 1. í´ë” ë‚´ íŒŒì¼ ì •ë³´ ì¡°íšŒ (ë³€ê²½ì´ë ¥ ì €ì¥ìš©)
        cur.execute("""
            SELECT doc_id, filename
            FROM pdf_documents
            WHERE member_id = %s AND filename LIKE %s
        """, (member_id, f"{folder_path}%"))
        files_to_delete = cur.fetchall()

        # ë³€ê²½ì´ë ¥ì— ì‚­ì œ ê¸°ë¡ ì¶”ê°€
        for doc_id, filename in files_to_delete:
            try:
                file_name = filename.split('/')[-1]
                cur.execute("""
                    INSERT INTO classification_history
                    (doc_id, file_name, full_path, original_folder, change_type)
                    VALUES (%s, %s, %s, %s, 'deleted')
                """, (doc_id, file_name, filename, filename))
            except Exception as history_error:
                print(f"âš ï¸  ë³€ê²½ì´ë ¥ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {history_error}")

        # 2. í´ë” ë‚´ ëª¨ë“  íŒŒì¼ ì‚­ì œ (DB)
        cur.execute("""
            DELETE FROM pdf_documents
            WHERE member_id = %s AND filename LIKE %s
        """, (member_id, f"{folder_path}%"))

        deleted_files_count = cur.rowcount

        # 3. í´ë” ì •ë³´ ì‚­ì œ (DB)
        cur.execute("""
            DELETE FROM folders
            WHERE member_id = %s AND folder_path LIKE %s
        """, (member_id, f"{folder_path}%"))

        deleted_folders_count = cur.rowcount

        # 4. ì‹¤ì œ íŒŒì¼ì‹œìŠ¤í…œì—ì„œ í´ë” ì‚­ì œ
        if os.path.exists(folder_path):
            shutil.rmtree(folder_path)

        conn.commit()

        return {
            "success": True,
            "message": f"í´ë” '{folder_name}'ì´(ê°€) ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤",
            "deleted_files": deleted_files_count,
            "deleted_folders": deleted_folders_count
        }

    except HTTPException as he:
        if conn:
            conn.rollback()
        raise he
    except Exception as e:
        if conn:
            conn.rollback()
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"í´ë” ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
    finally:
        if cur:
            cur.close()
        db_pool.release_conn(conn)


class RenameRequest(BaseModel):
    old_path: str
    new_name: str


@router.post("/rename_file")
async def rename_file(request: Request, rename_req: RenameRequest):
    """ê°œë³„ íŒŒì¼ ì´ë¦„ ë³€ê²½"""
    conn = db_pool.get_conn()
    cur = None

    try:
        if not request.session.get("user"):
            raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")

        cur = conn.cursor()
        member_id = request.session["user"]["member_id"]

        # ê¸°ì¡´ íŒŒì¼ ê²½ë¡œ
        old_path = rename_req.old_path

        # ìƒˆ íŒŒì¼ ê²½ë¡œ ìƒì„± (ê°™ì€ ë””ë ‰í† ë¦¬ì— ìƒˆ ì´ë¦„)
        directory = os.path.dirname(old_path)
        new_path = os.path.join(directory, rename_req.new_name).replace("\\", "/")

        # DBì—ì„œ íŒŒì¼ ì¡´ì¬ í™•ì¸
        cur.execute("""
            SELECT filename FROM pdf_documents
            WHERE member_id = %s AND filename = %s
        """, (member_id, old_path))

        if not cur.fetchone():
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ìƒˆ ì´ë¦„ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        cur.execute("""
            SELECT 1 FROM pdf_documents
            WHERE member_id = %s AND filename = %s
        """, (member_id, new_path))

        if cur.fetchone():
            raise HTTPException(status_code=400, detail="ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")

        # ì‹¤ì œ íŒŒì¼ ì´ë¦„ ë³€ê²½
        if os.path.exists(old_path):
            os.rename(old_path, new_path)

        # DB ì—…ë°ì´íŠ¸
        cur.execute("""
            UPDATE pdf_documents
            SET filename = %s, updated_at = %s
            WHERE member_id = %s AND filename = %s
        """, (new_path, datetime.now(), member_id, old_path))

        conn.commit()

        return {
            "success": True,
            "message": f"íŒŒì¼ ì´ë¦„ì´ '{rename_req.new_name}'(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤",
            "new_path": new_path
        }

    except HTTPException as he:
        if conn:
            conn.rollback()
        raise he
    except Exception as e:
        if conn:
            conn.rollback()
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {str(e)}")
    finally:
        if cur:
            cur.close()
        db_pool.release_conn(conn)


@router.post("/rename_folder")
async def rename_folder(request: Request, rename_req: RenameRequest):
    """í´ë” ì´ë¦„ ë³€ê²½"""
    conn = db_pool.get_conn()
    cur = None

    try:
        if not request.session.get("user"):
            raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")

        cur = conn.cursor()
        member_id = request.session["user"]["member_id"]
        username = request.session["user"]["name"]
        user_uid = request.session["user"]["id"]

        # ê¸°ì¡´ í´ë” ê²½ë¡œ
        old_path = rename_req.old_path

        # ìƒˆ í´ë” ê²½ë¡œ ìƒì„±
        parent_dir = os.path.dirname(old_path)
        new_path = os.path.join(parent_dir, rename_req.new_name).replace("\\", "/")

        # í´ë” ê²½ë¡œ ì •ê·œí™” (upload ì´í›„ ë¶€ë¶„ë§Œ)
        base_dir = os.path.join(".", "upload", username, user_uid).replace("\\", "/")

        # ì‹¤ì œ íŒŒì¼ì‹œìŠ¤í…œì—ì„œ í´ë” ì¡´ì¬ í™•ì¸
        if not os.path.exists(old_path):
            raise HTTPException(status_code=404, detail="í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ìƒˆ ì´ë¦„ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if os.path.exists(new_path):
            raise HTTPException(status_code=400, detail="ê°™ì€ ì´ë¦„ì˜ í´ë”ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")

        # ì‹¤ì œ íŒŒì¼ì‹œìŠ¤í…œì—ì„œ í´ë” ì´ë¦„ ë³€ê²½
        os.rename(old_path, new_path)

        # DBì—ì„œ í•´ë‹¹ í´ë”ì˜ ëª¨ë“  íŒŒì¼ ê²½ë¡œ ì—…ë°ì´íŠ¸
        cur.execute("""
            UPDATE pdf_documents
            SET filename = REPLACE(filename, %s, %s),
                updated_at = %s
            WHERE member_id = %s AND filename LIKE %s
        """, (old_path, new_path, datetime.now(), member_id, f"{old_path}%"))

        updated_files = cur.rowcount

        # folders í…Œì´ë¸”ë„ ì—…ë°ì´íŠ¸ (ìˆë‹¤ë©´)
        cur.execute("""
            UPDATE folders
            SET folder_path = REPLACE(folder_path, %s, %s),
                folder_name = %s
            WHERE member_id = %s AND folder_path LIKE %s
        """, (old_path, new_path, rename_req.new_name, member_id, f"{old_path}%"))

        conn.commit()

        return {
            "success": True,
            "message": f"í´ë” ì´ë¦„ì´ '{rename_req.new_name}'(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤",
            "updated_files": updated_files,
            "new_path": new_path
        }

    except HTTPException as he:
        if conn:
            conn.rollback()
        # íŒŒì¼ì‹œìŠ¤í…œ ë³€ê²½ ë¡¤ë°±
        if os.path.exists(new_path):
            os.rename(new_path, old_path)
        raise he
    except Exception as e:
        if conn:
            conn.rollback()
        # íŒŒì¼ì‹œìŠ¤í…œ ë³€ê²½ ë¡¤ë°±
        if 'new_path' in locals() and os.path.exists(new_path):
            try:
                os.rename(new_path, old_path)
            except:
                pass
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"í´ë” ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {str(e)}")
    finally:
        if cur:
            cur.close()
        db_pool.release_conn(conn)


@router.get("/files")
async def get_files(request: Request):
    conn = db_pool.get_conn()
    cur = conn.cursor()
    try:
        # ë¡œê·¸ì¸í•œ ì‚¬ìš©ì í™•ì¸
        if not request.session.get("user"):
            raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")

        userid = request.session["user"].get("member_id")

        # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cur.execute("""
            SELECT member_id, id, name
            FROM member_info
            WHERE member_id = %s
        """, (userid,))
        member_row = cur.fetchone()

        if not member_row:
            raise HTTPException(status_code=404, detail="ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        member_id, user_uid, username = member_row
        user_prefix = f"./upload/{username}/{user_uid}/"

        # í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ìì˜ íŒŒì¼ë§Œ ê°€ì ¸ì˜¤ê¸° (ë¶„ë¥˜ ì •ë³´ í¬í•¨)
        cur.execute("""
            SELECT
                p.doc_id,
                p.filename,
                p.upload_date,
                p.file_size,
                p.page_count,
                p.status,
                p.created_at,
                p.updated_at,
                p.member_id,
                m.name,
                m.email,
                p.ocr,
                p.is_classified,
                p.agency,
                p.document_type,
                p.confidence_agency,
                p.confidence_document_type,
                p.classified_date
            FROM pdf_documents p
            LEFT JOIN member_info m ON p.member_id = m.member_id
            WHERE p.member_id = %s
            ORDER BY p.created_at DESC
        """, (member_id,))
        rows = cur.fetchall()

        # íŒŒì¼ ê²½ë¡œì™€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        file_paths = []
        file_metadata = {}  # {ìƒëŒ€ê²½ë¡œ: {upload_date, file_size, full_path, ...}}

        for row in rows:
            full_path = row[1]
            if full_path.startswith(user_prefix):
                relative_path = full_path[len(user_prefix):]
                file_paths.append(relative_path)

                # ë©”íƒ€ë°ì´í„° ì €ì¥ (ì „ì²´ ê²½ë¡œ, OCR ìƒíƒœ, ë¶„ë¥˜ ì •ë³´ í¬í•¨)
                file_metadata[relative_path] = {
                    "doc_id": row[0],
                    "upload_date": row[2].isoformat() if row[2] else None,
                    "file_size": float(row[3]) if row[3] else 0,
                    "page_count": row[4],
                    "status": row[5],
                    "full_path": full_path,  # ì „ì²´ ê²½ë¡œ
                    "ocr_completed": row[11] if len(row) > 11 else False,  # OCR ì™„ë£Œ ì—¬ë¶€
                    "is_classified": row[12] if len(row) > 12 else False,  # ë¶„ë¥˜ ì™„ë£Œ ì—¬ë¶€
                    "agency": row[13] if len(row) > 13 else None,  # ê¸°ê´€
                    "document_type": row[14] if len(row) > 14 else None,  # ë¬¸ì„œìœ í˜•
                    "confidence_agency": row[15] if len(row) > 15 else None,  # ê¸°ê´€ ì‹ ë¢°ë„
                    "confidence_document_type": row[16] if len(row) > 16 else None,  # ë¬¸ì„œìœ í˜• ì‹ ë¢°ë„
                    "classified_date": row[17].isoformat() if len(row) > 17 and row[17] else None  # ë¶„ë¥˜ ì¼ì‹œ
                }

        # í´ë” ì •ë³´ë„ ê°€ì ¸ì˜¤ê¸° (ë¹ˆ í´ë” í¬í•¨)
        cur.execute("""
            SELECT folder_name, folder_path, created_at
            FROM folders
            WHERE member_id = %s
            ORDER BY created_at DESC
        """, (member_id,))
        folder_rows = cur.fetchall()

        # ë¹ˆ í´ë”ë¥¼ íŒŒì¼ ê²½ë¡œë¡œ ì¶”ê°€
        for folder_row in folder_rows:
            folder_path = folder_row[1]
            if folder_path.startswith(user_prefix):
                relative_folder = folder_path[len(user_prefix):]
                file_paths.append(relative_folder + "/.folder_placeholder")

        return {
            "file_paths": file_paths,
            "metadata": file_metadata
        }
    except HTTPException as he:
        raise he
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
    finally:
        if cur:
            cur.close()
        db_pool.release_conn(conn)

@router.post("/ocr/process")
async def process_ocr(filepath: str = Form(...)):
    """
    OCR ì²˜ë¦¬: PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

    Args:
        filepath: ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ

    Returns:
        OCR ê²°ê³¼ (í…ìŠ¤íŠ¸, í˜ì´ì§€ ì •ë³´)
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“„ OCR ì²˜ë¦¬ ì‹œì‘")
    print(f"   ìš”ì²­ ê²½ë¡œ: {filepath}")
    print(f"   í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"{'='*60}\n")

    if not OCR_AVAILABLE:
        print(f"âŒ OCR ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {"success": False, "error": "OCR service not available"}

    # íŒŒì¼ ê²½ë¡œ ì •ê·œí™”
    normalized_path = filepath.replace('\\', '/')

    # ./ ë¡œ ì‹œì‘í•˜ëŠ” ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if normalized_path.startswith('./'):
        normalized_path = normalized_path[2:]  # ./ ì œê±°

    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if not os.path.isabs(normalized_path):
        normalized_path = os.path.join(os.getcwd(), normalized_path)

    # ê²½ë¡œ ì •ê·œí™” (ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì œê±° ë“±)
    normalized_path = os.path.normpath(normalized_path)

    print(f"ğŸ” ì •ê·œí™”ëœ ê²½ë¡œ: {normalized_path}")
    print(f"ğŸ“‚ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(normalized_path)}")

    if not os.path.exists(normalized_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {normalized_path}")
        # ê°€ëŠ¥í•œ ê²½ë¡œë“¤ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        possible_paths = [
            os.path.join(os.getcwd(), filepath),
            os.path.join(os.getcwd(), filepath.lstrip('./')),
            filepath
        ]
        print(f"   ì‹œë„í•œ ê²½ë¡œë“¤:")
        for p in possible_paths:
            print(f"     - {p} (ì¡´ì¬: {os.path.exists(p)})")
        return {"success": False, "error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}"}

    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì›ë³¸ ê²½ë¡œë¡œ DB ì¡°íšŒ)
        print(f"ğŸ” DB ì¡°íšŒ ì¤‘... ê²½ë¡œ: {filepath}")
        cur.execute("SELECT doc_id, page_count FROM pdf_documents WHERE filename = %s", (filepath,))
        row = cur.fetchone()

        if not row:
            print(f"âŒ DBì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return {"success": False, "message": f"íŒŒì¼ {filepath} ì´(ê°€) DBì— ì—†ìŠµë‹ˆë‹¤."}

        doc_id = row[0]
        page_count = row[1]
        print(f"âœ… DBì—ì„œ íŒŒì¼ ë°œê²¬ - doc_id: {doc_id}, í˜ì´ì§€ ìˆ˜: {page_count}")

        # OCR ì²˜ë¦¬ ì‹œì‘
        print(f"ğŸš€ OCR ì—”ì§„ ì‹œì‘...")
        start_time = time.time()

        try:
            # Context managerë¡œ OCR ì„œë¹„ìŠ¤ ì‚¬ìš© - ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ í•´ì œ
            with get_ocr_service() as ocr:
                full_text, page_data = ocr.extract_text_from_pdf(normalized_path)

            processing_time = time.time() - start_time
            print(f"âœ… OCR ì™„ë£Œ - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ, ì¶”ì¶œëœ í˜ì´ì§€: {len(page_data)}ê°œ")

            # OCR ê²°ê³¼ DB ì €ì¥
            cur.execute("""
                INSERT INTO ocr_results (doc_id, full_text, page_data, ocr_engine, processing_time)
                VALUES (%s, %s, %s, %s, %s)
                RETURNING ocr_id
            """, (doc_id, full_text, json.dumps(page_data, ensure_ascii=False), "PaddleOCRVL", processing_time))

            ocr_id = cur.fetchone()[0]

            # PDF ë¬¸ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸
            cur.execute("""
                UPDATE pdf_documents
                SET ocr = TRUE, status = 'OCR_COMPLETED', updated_at = NOW()
                WHERE filename = %s
            """, (filepath,))

            conn.commit()

            # ì‘ì—… ë¡œê·¸ ê¸°ë¡
            log_processing(
                conn=conn,
                doc_id=doc_id,
                filename=filepath,
                process_type='OCR',
                status='SUCCESS',
                message=f"OCR ì²˜ë¦¬ ì™„ë£Œ: {filepath.split('/')[-1]} ({len(page_data)}í˜ì´ì§€)"
            )

            print(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ - ocr_id: {ocr_id}")
            print(f"{'='*60}\n")

            return {
                "success": True,
                "message": f"OCR ì™„ë£Œ: {filepath}",
                "ocr_id": ocr_id,
                "doc_id": doc_id,
                "processing_time": processing_time,
                "page_count": len(page_data),
                "text_preview": full_text[:200] if full_text else ""
            }

        except Exception as ocr_error:
            conn.rollback()
            print(f"âŒ OCR ì—”ì§„ ì˜¤ë¥˜: {str(ocr_error)}")
            import traceback
            traceback.print_exc()
            return {"success": False, "error": f"OCR ì²˜ë¦¬ ì‹¤íŒ¨: {str(ocr_error)}"}

    except Exception as e:
        conn.rollback()
        print(f"âŒ OCR ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


@router.post("/ocrcompleted")
async def ocrcomplet(filepath: str = Form(...)):
    """
    OCR ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ìœ ì§€)
    """
    print(f"ğŸ“„ OCR ì™„ë£Œëœ íŒŒì¼ ê²½ë¡œ: {filepath}")
    conn = db_pool.get_conn()
    cur = conn.cursor()
    try:
        cur.execute("SELECT 1 FROM pdf_documents WHERE filename = %s", (filepath,))
        exists = cur.fetchone()

        if not exists:
            return {"success": False, "message": f"íŒŒì¼ {filepath} ì´(ê°€) DBì— ì—†ìŠµë‹ˆë‹¤."}

        cur.execute("""
            UPDATE pdf_documents
            SET ocr = TRUE, updated_at = NOW()
            WHERE filename = %s
        """, (filepath,))
        conn.commit()

        return {"success": True, "message": f"OCR ì™„ë£Œ ì²˜ë¦¬ë¨: {filepath}"}
    except Exception as e:
        conn.rollback()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


@router.post("/classify/document")
async def classify_document(
    doc_id: Optional[int] = Form(None),
    file_path: Optional[str] = Form(None)
):
    """
    ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (ê¸°ê´€, ë¬¸ì„œìœ í˜•)

    Args:
        doc_id: ë¶„ë¥˜í•  ë¬¸ì„œ ID (ì„ íƒ)
        file_path: ë¶„ë¥˜í•  ë¬¸ì„œ ê²½ë¡œ (ì„ íƒ)

    Note:
        doc_id ë˜ëŠ” file_path ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.

    Returns:
        ë¶„ë¥˜ ê²°ê³¼ (ê¸°ê´€, ë¬¸ì„œìœ í˜•, ì‹ ë¢°ë„)
    """
    if not doc_id and not file_path:
        return {"success": False, "error": "doc_id ë˜ëŠ” file_pathê°€ í•„ìš”í•©ë‹ˆë‹¤"}

    print(f"\n{'='*60}")
    print(f"ğŸ“‹ ë¬¸ì„œ ë¶„ë¥˜ ì‹œì‘: doc_id={doc_id}, file_path={file_path}")
    print(f"{'='*60}\n")

    if not CLASSIFICATION_AVAILABLE:
        print(f"âŒ ë¶„ë¥˜ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {"success": False, "error": "Classification service not available"}

    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # file_pathë¡œë¶€í„° doc_id ì¡°íšŒ
        if not doc_id and file_path:
            print(f"ğŸ” file_pathë¡œ doc_id ì¡°íšŒ ì¤‘: {file_path}")

            # ê²½ë¡œ ì •ê·œí™”
            normalized_path = file_path.replace('\\', '/')
            if normalized_path.startswith('./'):
                normalized_path = normalized_path[2:]

            cur.execute("""
                SELECT doc_id
                FROM pdf_documents
                WHERE filename = %s OR filename LIKE %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (normalized_path, f"%{normalized_path}"))

            row = cur.fetchone()
            if not row:
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return {"success": False, "error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}

            doc_id = row[0]
            print(f"âœ… doc_id ë°œê²¬: {doc_id}")

        # OCR ê²°ê³¼ ì¡°íšŒ
        print(f"ğŸ” OCR ê²°ê³¼ ì¡°íšŒ ì¤‘... doc_id={doc_id}")
        cur.execute("""
            SELECT ocr_id, full_text
            FROM ocr_results
            WHERE doc_id = %s
            ORDER BY created_at DESC
            LIMIT 1
        """, (doc_id,))

        row = cur.fetchone()
        if not row:
            print(f"âŒ OCR ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: doc_id={doc_id}")
            return {"success": False, "error": f"OCR ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: doc_id={doc_id}"}

        ocr_id, full_text = row
        print(f"âœ… OCR ê²°ê³¼ ë°œê²¬ - ocr_id={ocr_id}, í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text)} ì")

        if not full_text or full_text.strip() == "":
            print(f"âŒ OCR í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return {"success": False, "error": "OCR í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

        # ë¶„ë¥˜ ì‹¤í–‰
        print(f"ğŸš€ BERT ë¶„ë¥˜ ëª¨ë¸ ì‹¤í–‰ ì¤‘...")
        start_time = time.time()

        # Context managerë¡œ BERT ë¶„ë¥˜ ì„œë¹„ìŠ¤ ì‚¬ìš© - ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ í•´ì œ
        with get_classification_service() as classifier:
            classification_result = classifier.predict(full_text, return_probs=True)

        processing_time = time.time() - start_time
        print(f"âœ… ë¶„ë¥˜ ì™„ë£Œ - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        print(f"   ê¸°ê´€: {classification_result.get('ê¸°ê´€')} (ì‹ ë¢°ë„: {classification_result.get('confidence', {}).get('ê¸°ê´€', 0):.2%})")
        print(f"   ë¬¸ì„œìœ í˜•: {classification_result.get('ë¬¸ì„œìœ í˜•')} (ì‹ ë¢°ë„: {classification_result.get('confidence', {}).get('ë¬¸ì„œìœ í˜•', 0):.2%})")

        # ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ (DOCUMENT_KEYWORDS í…Œì´ë¸” í™œìš©)
        cur.execute("""
            INSERT INTO document_keywords (doc_id, keywords, main_topic, keyword_count, raw_response, model_name)
            VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING keyword_id
        """, (
            doc_id,
            json.dumps(classification_result, ensure_ascii=False),  # keywords í•„ë“œì— ì „ì²´ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
            f"ê¸°ê´€: {classification_result.get('ê¸°ê´€', 'Unknown')}, ë¬¸ì„œìœ í˜•: {classification_result.get('ë¬¸ì„œìœ í˜•', 'Unknown')}",
            len(classification_result.get('probabilities', {}).get('ê¸°ê´€', {})),
            json.dumps(classification_result.get('probabilities', {}), ensure_ascii=False),
            "2-Task-BERT"
        ))

        keyword_id = cur.fetchone()[0]

        # ë¶„ë¥˜ ì •ë³´ ì»¬ëŸ¼ ì¶”ê°€ (ì—†ìœ¼ë©´)
        try:
            cur.execute("""
                ALTER TABLE pdf_documents
                ADD COLUMN IF NOT EXISTS agency VARCHAR(200),
                ADD COLUMN IF NOT EXISTS document_type VARCHAR(200),
                ADD COLUMN IF NOT EXISTS confidence_agency FLOAT,
                ADD COLUMN IF NOT EXISTS confidence_document_type FLOAT,
                ADD COLUMN IF NOT EXISTS is_classified BOOLEAN DEFAULT FALSE,
                ADD COLUMN IF NOT EXISTS classified_date TIMESTAMP
            """)
        except:
            pass  # ì´ë¯¸ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ë¬´ì‹œ

        # ë¬¸ì„œ ìƒíƒœ ë° ë¶„ë¥˜ ì •ë³´ ì—…ë°ì´íŠ¸
        cur.execute("""
            UPDATE pdf_documents
            SET status = 'CLASSIFIED',
                agency = %s,
                document_type = %s,
                confidence_agency = %s,
                confidence_document_type = %s,
                is_classified = TRUE,
                classified_date = NOW(),
                updated_at = NOW()
            WHERE doc_id = %s
        """, (
            classification_result.get('ê¸°ê´€', 'Unknown'),
            classification_result.get('ë¬¸ì„œìœ í˜•', 'Unknown'),
            classification_result.get('confidence', {}).get('ê¸°ê´€', 0.0),
            classification_result.get('confidence', {}).get('ë¬¸ì„œìœ í˜•', 0.0),
            doc_id
        ))

        conn.commit()

        # ì‘ì—… ë¡œê·¸ ê¸°ë¡
        cur.execute("SELECT filename FROM pdf_documents WHERE doc_id = %s", (doc_id,))
        filename_row = cur.fetchone()
        filename = filename_row[0] if filename_row else "Unknown"

        log_processing(
            conn=conn,
            doc_id=doc_id,
            filename=filename,
            process_type='CLASSIFICATION',
            status='SUCCESS',
            message=f"BERT ë¶„ë¥˜ ì™„ë£Œ: {filename.split('/')[-1]} - {classification_result.get('ê¸°ê´€', 'Unknown')}/{classification_result.get('ë¬¸ì„œìœ í˜•', 'Unknown')}"
        )

        print(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ - keyword_id: {keyword_id}")
        print(f"{'='*60}\n")

        return {
            "success": True,
            "doc_id": doc_id,
            "keyword_id": keyword_id,
            "classification": {
                "ê¸°ê´€": classification_result.get('ê¸°ê´€'),
                "ë¬¸ì„œìœ í˜•": classification_result.get('ë¬¸ì„œìœ í˜•'),
                "confidence": classification_result.get('confidence', {}),
                "probabilities": classification_result.get('probabilities', {}) if 'probabilities' in classification_result else None
            },
            "processing_time": processing_time
        }

    except Exception as e:
        conn.rollback()
        print(f"âŒ ë¬¸ì„œ ë¶„ë¥˜ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


@router.get("/classification/{doc_id}")
async def get_classification_result(doc_id: int):
    """
    ë¬¸ì„œ ë¶„ë¥˜ ê²°ê³¼ ì¡°íšŒ

    Args:
        doc_id: ë¬¸ì„œ ID

    Returns:
        ë¶„ë¥˜ ê²°ê³¼
    """
    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        cur.execute("""
            SELECT keyword_id, keywords, main_topic, raw_response, model_name, created_at
            FROM document_keywords
            WHERE doc_id = %s
            ORDER BY created_at DESC
            LIMIT 1
        """, (doc_id,))

        row = cur.fetchone()
        if not row:
            return {"success": False, "error": f"ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: doc_id={doc_id}"}

        keyword_id, keywords, main_topic, raw_response, model_name, created_at = row

        # JSON íŒŒì‹±
        try:
            keywords_data = json.loads(keywords) if keywords else {}
            probabilities = json.loads(raw_response) if raw_response else {}
        except:
            keywords_data = {}
            probabilities = {}

        return {
            "success": True,
            "doc_id": doc_id,
            "keyword_id": keyword_id,
            "ê¸°ê´€": keywords_data.get('ê¸°ê´€'),
            "ë¬¸ì„œìœ í˜•": keywords_data.get('ë¬¸ì„œìœ í˜•'),
            "confidence": keywords_data.get('confidence', {}),
            "probabilities": probabilities,
            "model_name": model_name,
            "created_at": created_at.isoformat() if created_at else None
        }

    except Exception as e:
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


@router.get("/classification/results/all")
async def get_all_classification_results(limit: int = 100, offset: int = 0):
    """
    ëª¨ë“  ë¶„ë¥˜ ê²°ê³¼ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜)

    Args:
        limit: ì¡°íšŒí•  ìµœëŒ€ ê°œìˆ˜
        offset: ì‹œì‘ ìœ„ì¹˜

    Returns:
        ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
    """
    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # ë¶„ë¥˜ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        cur.execute("""
            SELECT
                p.doc_id,
                p.filename,
                p.file_size,
                p.page_count,
                p.upload_date,
                k.keyword_id,
                k.keywords,
                k.main_topic,
                k.raw_response,
                k.created_at as classified_at,
                o.full_text
            FROM pdf_documents p
            INNER JOIN document_keywords k ON p.doc_id = k.doc_id
            LEFT JOIN ocr_results o ON p.doc_id = o.doc_id
            ORDER BY k.created_at DESC
            LIMIT %s OFFSET %s
        """, (limit, offset))

        rows = cur.fetchall()
        results = []

        for row in rows:
            doc_id, filename, file_size, page_count, upload_date, keyword_id, keywords, main_topic, raw_response, classified_at, full_text = row

            # JSON íŒŒì‹±
            try:
                keywords_data = json.loads(keywords) if keywords else {}
                probabilities = json.loads(raw_response) if raw_response else {}
            except:
                keywords_data = {}
                probabilities = {}

            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = keywords_data.get('confidence', {})
            avg_confidence = sum(confidence.values()) / len(confidence) if confidence else 0

            # íŒŒì¼ëª…ì—ì„œ ê²½ë¡œ ì œê±°
            display_filename = filename.split('/')[-1].split('\\')[-1]

            results.append({
                "doc_id": doc_id,
                "filename": display_filename,
                "full_path": filename,
                "file_size": file_size,
                "page_count": page_count,
                "upload_date": upload_date.isoformat() if upload_date else None,
                "keyword_id": keyword_id,
                "ê¸°ê´€": keywords_data.get('ê¸°ê´€'),
                "ë¬¸ì„œìœ í˜•": keywords_data.get('ë¬¸ì„œìœ í˜•'),
                "confidence": confidence,
                "avg_confidence": round(avg_confidence, 4),
                "needs_review": avg_confidence < 0.7,
                "probabilities": probabilities,
                "main_topic": main_topic,
                "classified_at": classified_at.isoformat() if classified_at else None,
                "text_preview": full_text[:200] if full_text else ""
            })

        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        cur.execute("SELECT COUNT(*) FROM document_keywords")
        total_count = cur.fetchone()[0]

        return {
            "success": True,
            "results": results,
            "total_count": total_count,
            "limit": limit,
            "offset": offset
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)



# @router.post("/upload")
# async def upload_file(file: UploadFile = File(...)):
#     os.makedirs(f"{file.id}", exist_ok=True)
#     save_path = f"uploads/{file.filename}"
#     save_size = os.path.getsize(file.filename)
#     print("ë„ˆëŠ” ê²½ë¡œê°€??????",save_path)
#     with open(save_path, "wb") as buffer:
#         shutil.copyfileobj(file.file, buffer)

#     ext = file.filename.split(".")[-1].lower()
#     conn = db_pool.get_conn()
#     cursor = conn.cursor()

#     try:
#         if ext == "zip":
#             added = extract_zip(save_path, file.filename)
#             return {"message": f"ZIP ì—…ë¡œë“œ ì™„ë£Œ ({added}ê°œ ì¶”ê°€ë¨)", "file_count": added}
#         elif ext == "pdf":
#             cursor.execute(
#                 "SELECT 1 FROM pdf_documents WHERE filename = %s", (file.filename,)
#             )
#             if cursor.fetchone():
#                 return {"message": "ì´ë¯¸ ì—…ë¡œë“œëœ íŒŒì¼ì…ë‹ˆë‹¤.", "file_count": 0}

#             cursor.execute("""
#                 INSERT INTO pdf_documents (file_size ,filename, status, created_at, updated_at)
#                 VALUES (%s,%s, %s, %s, %s)
#             """, (save_size,save_path, "uploaded", datetime.now(), datetime.now()))
#             conn.commit()
#             return {"message": "PDF ì—…ë¡œë“œ ì™„ë£Œ", "file_count": 1}
#         else:
#             return {"error": "zip ë˜ëŠ” pdfë§Œ ì—…ë¡œë“œ ê°€ëŠ¥"}
#     except Exception as e:
#         conn.rollback()
#         return {"error": str(e)}
#     finally:    
#         cursor.close()
#         db_pool.release_conn(conn)




# ===== ë¡œê·¸ì¸ ëª¨ë¸ =====
class LoginRequest(BaseModel):
    id: str
    password: str

# ë¡œê·¸ì¸
@auth_router.post("/login")
def login_endpoint(data: LoginRequest, request: Request):
    result = login_member(data.id, data.password, request.session)
    if "error" in result:
        raise HTTPException(status_code=401, detail=result["error"])
    return result

# ë¡œê·¸ì•„ì›ƒ
@auth_router.get("/logout")
def logout_endpoint(request: Request):
    return logout_member(request.session)


@auth_router.get("/me")
def get_current_user_endpoint(request: Request):
    result = get_current_user(request.session)
    if "error" in result:
        raise HTTPException(status_code=401, detail=result["error"])
    return result


# ===== íšŒì› ëª¨ë¸ =====
# íšŒì›ê°€ì… ëª¨ë¸
class AddMemberRequest(BaseModel):
    id: str
    password: str
    name: str
    phone: str
    email: str
    member_role: str = 'R2'
    member_grade: str = 'G2'

# íšŒì›ì •ë³´ìˆ˜ì • ëª¨ë¸
class UpdateMemberRequest(BaseModel):
    id: str
    name: str | None = None
    phone: str | None = None
    email: str | None = None
    password: str | None = None
    member_role: str | None = None
    member_grade: str | None = None

# íšŒì›ê°€ì…
@auth_router.post("/member/add")
def add_member_endpoint(data: AddMemberRequest):
    try:
        member_id = add_member(
            id=data.id,
            password=data.password,
            name=data.name,
            phone=data.phone,
            email=data.email,
            member_role=data.member_role,
            member_grade=data.member_grade
        )
        return {"message": "Member added successfully", "member_id": member_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# íšŒì› ì •ë³´ ì¡°íšŒìš©
@auth_router.get("/member/me")
def get_my_member_info(request: Request):
    session_user = get_current_user(request.session)
    if "error" in session_user:
        raise HTTPException(status_code=401, detail=session_user["error"])

    member_id = session_user["member_id"]
    member = get_member_by_id(member_id)  # member.py í•¨ìˆ˜ ì‚¬ìš©
    if not member:
        raise HTTPException(status_code=404, detail="Member not found")

    return member  # ì‹¤ì œ íšŒì› ì •ë³´ ë°˜í™˜

# íšŒì›ì •ë³´ìˆ˜ì •
@auth_router.put("/member/update")
def update_member_endpoint(data: UpdateMemberRequest):
    success = update_member(
        id=data.id,
        name=data.name,
        phone=data.phone,
        email=data.email,
        password=data.password,
        member_role=data.member_role,
        member_grade=data.member_grade
    )
    if not success:
        raise HTTPException(status_code=400, detail="No fields to update or member not found")
    return {"message": "Member updated successfully"}

# íšŒì›ì‚­ì œ
@auth_router.delete("/member/delete/{member_id}")
def delete_member_endpoint(member_id: str, response: Response):
    success = delete_member(member_id)
    if not success:
        raise HTTPException(status_code=404, detail="Member not found")

    # ì„¸ì…˜ ì¿ í‚¤ ì‚­ì œ â†’ ë¸Œë¼ìš°ì €ì—ì„œ ë¡œê·¸ì•„ì›ƒ ì²˜ë¦¬
    response.delete_cookie(key="session")

    return {"message": "Member deleted successfully"}

# íšŒì› ì •ë³´ ì¡°íšŒ - ê´€ë¦¬ììš©
@auth_router.get("/member/admin/{member_id}")
def get_member_endpoint_admin(member_id: str):
    member = get_member_by_id(member_id)
    if not member:
        raise HTTPException(status_code=404, detail="Member not found")
    return member

# ì „ì²´ íšŒì›ìˆ˜ ì¡°íšŒ í•˜ëŠ” ì½”ë“œ
@auth_router.get("/member/count")
def get_member_count():
    """
    ì „ì²´ íšŒì› ìˆ˜ ì¡°íšŒ API (member_role='R2'ë§Œ)
    """
    try:
        total = get_total_member_count()
        return {"total_members": total}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/statistics/dashboard")
async def get_dashboard_statistics():
    """
    ëŒ€ì‹œë³´ë“œ í†µê³„ ë°ì´í„° ì¡°íšŒ
    - ì´ ë¬¸ì„œ ìˆ˜
    - ì´ ì¸ë±ì‹± ìš©ëŸ‰
    - OCR ì™„ë£Œ ë¬¸ì„œ ìˆ˜
    - ë¶„ë¥˜ ì™„ë£Œ ë¬¸ì„œ ìˆ˜
    - ê¸ˆì¼ ì—…ë¡œë“œ ë¬¸ì„œ ìˆ˜
    """
    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # ì´ ë¬¸ì„œ ìˆ˜ ë° ì´ ìš©ëŸ‰
        cur.execute("""
            SELECT
                COUNT(*) as total_docs,
                COALESCE(SUM(file_size), 0) as total_size
            FROM pdf_documents
        """)
        row = cur.fetchone()
        total_docs = row[0] if row else 0
        total_size = float(row[1]) if row else 0.0

        # OCR ì™„ë£Œ ë¬¸ì„œ ìˆ˜
        cur.execute("SELECT COUNT(*) FROM pdf_documents WHERE ocr = TRUE")
        ocr_completed = cur.fetchone()[0]

        # ë¶„ë¥˜ ì™„ë£Œ ë¬¸ì„œ ìˆ˜ (pdf_documents.is_classified ì‚¬ìš©)
        cur.execute("SELECT COUNT(*) FROM pdf_documents WHERE is_classified = TRUE")
        classified_docs = cur.fetchone()[0]

        # ê¸ˆì¼ ì—…ë¡œë“œ ë¬¸ì„œ ìˆ˜
        cur.execute("""
            SELECT COUNT(*)
            FROM pdf_documents
            WHERE DATE(upload_date) = CURRENT_DATE
        """)
        today_uploads = cur.fetchone()[0]

        # ê¸ˆì¼ ì—…ë°ì´íŠ¸ ë¬¸ì„œ ìˆ˜ (OCR ë˜ëŠ” ë¶„ë¥˜ ì™„ë£Œ)
        cur.execute("""
            SELECT COUNT(*)
            FROM pdf_documents
            WHERE DATE(updated_at) = CURRENT_DATE
            AND (ocr = TRUE OR status LIKE '%CLASSIFIED%')
        """)
        today_updates = cur.fetchone()[0]

        # ìµœê·¼ 7ì¼ê°„ ì¼ë³„ ì‹ ê·œ ë“±ë¡ ë° ì—…ë°ì´íŠ¸ í†µê³„
        cur.execute("""
            WITH days AS (
                SELECT generate_series(
                    CURRENT_DATE - INTERVAL '6 days',
                    CURRENT_DATE,
                    '1 day'::interval
                )::date AS day
            )
            SELECT
                TO_CHAR(d.day, 'Dy') as day_name,
                COALESCE(COUNT(p.doc_id) FILTER (WHERE DATE(p.upload_date) = d.day), 0) as uploads,
                COALESCE(COUNT(p.doc_id) FILTER (WHERE DATE(p.updated_at) = d.day AND p.upload_date < d.day), 0) as updates
            FROM days d
            LEFT JOIN pdf_documents p ON DATE(p.upload_date) = d.day OR DATE(p.updated_at) = d.day
            GROUP BY d.day
            ORDER BY d.day
        """)

        weekly_data = []
        day_names_kr = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        day_names_en = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']

        for row in cur.fetchall():
            day_name_en = row[0]
            # ì˜ë¬¸ ìš”ì¼ì„ í•œê¸€ë¡œ ë³€í™˜
            try:
                day_index = day_names_en.index(day_name_en)
                day_name_kr = day_names_kr[day_index]
            except:
                day_name_kr = day_name_en

            weekly_data.append({
                "name": day_name_kr,
                "ì‹ ê·œë“±ë¡": int(row[1]) if row[1] else 0,
                "ì—…ë°ì´íŠ¸": int(row[2]) if row[2] else 0
            })

        return {
            "success": True,
            "total_documents": total_docs,
            "total_size_mb": round(total_size, 2),
            "total_size_gb": round(total_size / 1024, 2),
            "ocr_completed": ocr_completed,
            "classified_documents": classified_docs,
            "today_uploads": today_uploads,
            "today_updates": today_updates,
            "weekly_data": weekly_data
        }

    except Exception as e:
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


@router.get("/statistics/processing-logs")
async def get_processing_logs(limit: int = 10):
    """
    ìµœê·¼ ì²˜ë¦¬ ë¡œê·¸ ì¡°íšŒ (AI ì‘ì—… ì´ë ¥ìš©)
    - processing_log í…Œì´ë¸”ì—ì„œ ì‹¤ì œ ì‘ì—… ì´ë ¥ ì¡°íšŒ
    """
    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # processing_log í…Œì´ë¸” ìƒì„± (ì—†ìœ¼ë©´)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS processing_log (
                log_id SERIAL PRIMARY KEY,
                doc_id INTEGER,
                process_type VARCHAR(50) NOT NULL,
                status VARCHAR(50) NOT NULL,
                message TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (doc_id) REFERENCES pdf_documents(doc_id) ON DELETE CASCADE
            )
        """)

        # filename ì»¬ëŸ¼ ì¶”ê°€ (ì—†ìœ¼ë©´)
        cur.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name='processing_log' AND column_name='filename'
        """)
        if not cur.fetchone():
            cur.execute("ALTER TABLE processing_log ADD COLUMN filename VARCHAR(500)")
            print("âœ… processing_log í…Œì´ë¸”ì— filename ì»¬ëŸ¼ ì¶”ê°€")

        # message ì»¬ëŸ¼ ì¶”ê°€ (ì—†ìœ¼ë©´)
        cur.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name='processing_log' AND column_name='message'
        """)
        if not cur.fetchone():
            cur.execute("ALTER TABLE processing_log ADD COLUMN message TEXT")
            print("âœ… processing_log í…Œì´ë¸”ì— message ì»¬ëŸ¼ ì¶”ê°€")

        conn.commit()

        # processing_log í…Œì´ë¸”ì—ì„œ ìµœê·¼ ë¡œê·¸ ì¡°íšŒ
        cur.execute("""
            SELECT
                log_id,
                doc_id,
                filename,
                process_type,
                status,
                message,
                created_at
            FROM processing_log
            ORDER BY created_at DESC
            LIMIT %s
        """, (limit,))

        rows = cur.fetchall()
        logs = []

        for row in rows:
            log_id, doc_id, filename, process_type, status, message, created_at = row

            logs.append({
                "log_id": log_id,
                "doc_id": doc_id,
                "filename": filename,
                "process_type": process_type,
                "status": status,
                "message": message,
                "timestamp": created_at.isoformat() if created_at else None
            })

        return {
            "success": True,
            "logs": logs
        }

    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)

# íšŒì› ì •ë³´ ì¡°íšŒ
@auth_router.get("/member/{member_id}")
def get_member_endpoint(member_id: str):
    member = get_member_by_id(member_id)
    if not member:
        raise HTTPException(status_code=404, detail="Member not found")
    return member


# ============================================================================
# ì¹´í…Œê³ ë¦¬ ìë™ ìƒì„± API (Gemma3)
# ============================================================================

@router.post("/category/auto-generate")
async def auto_generate_categories(request: Request):
    """
    Gemma3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ OCR ì™„ë£Œ íŒŒì¼ë“¤ë¡œë¶€í„° ì¹´í…Œê³ ë¦¬ êµ¬ì¡° ìë™ ìƒì„±

    Request Body:
        {
            "files": ["path1", "path2", ...],
            "level": 1~4 (ì¹´í…Œê³ ë¦¬ ìµœëŒ€ ë‹¨ê³„)
        }

    Returns:
        {
            "success": bool,
            "categories": {...},  # ìƒì„±ëœ ì¹´í…Œê³ ë¦¬ êµ¬ì¡°
            "classified_documents": {...}  # ê° ë¬¸ì„œì˜ ì¹´í…Œê³ ë¦¬ ë°°ì¹˜
        }
    """
    try:
        data = await request.json()
        files = data.get('files', [])
        level = data.get('level', 2)

        print(f"\n{'='*60}")
        print(f"ğŸ¤– Gemma3 ì¹´í…Œê³ ë¦¬ ìë™ ìƒì„± ì‹œì‘")
        print(f"   íŒŒì¼ ê°œìˆ˜: {len(files)}")
        print(f"   ìµœëŒ€ ë‹¨ê³„: {level}")
        print(f"{'='*60}\n")

        if not files:
            return {"success": False, "error": "íŒŒì¼ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

        conn = db_pool.get_conn()
        cur = conn.cursor()

        # 1. ê° íŒŒì¼ì˜ OCR í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        documents = []
        for file_path in files:
            print(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_path}")

            # ê²½ë¡œ ì •ê·œí™”
            normalized_path = file_path.replace('\\', '/')
            if normalized_path.startswith('./'):
                normalized_path = normalized_path[2:]

            # doc_id ì¡°íšŒ
            cur.execute("""
                SELECT doc_id
                FROM pdf_documents
                WHERE filename = %s OR filename LIKE %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (normalized_path, f"%{normalized_path}"))

            row = cur.fetchone()
            if not row:
                print(f"âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                continue

            doc_id = row[0]

            # OCR í…ìŠ¤íŠ¸ ì¡°íšŒ
            cur.execute("""
                SELECT full_text
                FROM ocr_results
                WHERE doc_id = %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (doc_id,))

            ocr_row = cur.fetchone()
            if ocr_row and ocr_row[0]:
                documents.append({
                    "doc_id": doc_id,
                    "file_path": file_path,
                    "text": ocr_row[0][:1000]  # ì²˜ìŒ 1000ìë§Œ ì‚¬ìš© (ì†ë„ í–¥ìƒ)
                })
                print(f"âœ… OCR í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ (doc_id={doc_id})")

        if not documents:
            return {"success": False, "error": "OCR í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        print(f"\nğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ\n")

        # 2. ê° ë¬¸ì„œë¥¼ ê°œë³„ì ìœ¼ë¡œ Gemma3ë¡œ ë¶„ë¥˜
        print("ğŸ¤– Gemma3 ëª¨ë¸ë¡œ ë¬¸ì„œ ê°œë³„ ë¶„ë¥˜ ì¤‘...")

        import requests
        classified_documents = {}
        categories = {}

        for idx, doc in enumerate(documents):
            print(f"\n[{idx+1}/{len(documents)}] ë¬¸ì„œ ë¶„ë¥˜ ì¤‘: {doc['file_path'].split('/')[-1]}")

            try:
                # levelì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
                if level == 1:
                    level_instruction = """ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
- subcategory, detail, subdetail í•„ë“œë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- category í•„ë“œë§Œ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”"""
                    example_format = '{"category": "ë²•ì œì‚¬ë²•ìœ„ì›íšŒ"}'
                elif level == 2:
                    level_instruction = """2ë‹¨ê³„ êµ¬ì¡°ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
- categoryì™€ subcategory í•„ë“œë§Œ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
- detail, subdetail í•„ë“œë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""
                    example_format = '{"category": "ë²•ì œì‚¬ë²•ìœ„ì›íšŒ", "subcategory": "ê²€í† ë³´ê³ ì„œ"}'
                elif level == 3:
                    level_instruction = """3ë‹¨ê³„ êµ¬ì¡°ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
- category, subcategory, detail í•„ë“œë¥¼ ë°˜ë“œì‹œ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”
- subdetail í•„ë“œë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ê° ë‹¨ê³„ë³„ë¡œ êµ¬ì²´ì ì¸ ë¶„ë¥˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”"""
                    example_format = '{"category": "ë²•ì œì‚¬ë²•ìœ„ì›íšŒ", "subcategory": "ê²€í† ë³´ê³ ì„œ", "detail": "ë²•ë¥ ì•ˆ"}'
                else:
                    level_instruction = """4ë‹¨ê³„ êµ¬ì¡°ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
- category, subcategory, detail, subdetail í•„ë“œë¥¼ ë°˜ë“œì‹œ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”
- ê° ë‹¨ê³„ë³„ë¡œ êµ¬ì²´ì ì¸ ë¶„ë¥˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”"""
                    example_format = '{"category": "ë²•ì œì‚¬ë²•ìœ„ì›íšŒ", "subcategory": "ê²€í† ë³´ê³ ì„œ", "detail": "ë²•ë¥ ì•ˆ", "subdetail": "2024ë…„"}'

                # Gemma3 í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ OCR í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ì— ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{doc['text'][:800]}

ìš”êµ¬ì‚¬í•­:
1. ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ì„±ê²©ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”
2. {level_instruction}
3. ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš” (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):

{example_format}

JSON ì‘ë‹µ:"""

                # Ollama API í˜¸ì¶œ
                ollama_url = "http://localhost:11434/api/generate"
                ollama_payload = {
                    "model": "gemma3:4b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # ë” ê²°ì •ì ì¸ ì¶œë ¥ì„ ìœ„í•´ ë‚®ì¶¤
                        "top_p": 0.9,
                        "num_predict": 200  # JSON ì‘ë‹µì— ì¶©ë¶„í•œ ê¸¸ì´
                    }
                }

                response = requests.post(ollama_url, json=ollama_payload, timeout=30)

                if response.status_code != 200:
                    raise Exception(f"Ollama API ì˜¤ë¥˜: {response.status_code}")

                result = response.json()
                gemma_response = result.get('response', '')

                # JSON íŒŒì‹±
                import re
                json_match = re.search(r'\{.*\}', gemma_response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    json_str = gemma_response

                classification = json.loads(json_str)

                category = classification.get('category', 'ì¼ë°˜ë¬¸ì„œ')
                subcategory = classification.get('subcategory')
                detail = classification.get('detail')
                subdetail = classification.get('subdetail')

                # ì¹´í…Œê³ ë¦¬ êµ¬ì¡°ì— ì¶”ê°€
                if category not in categories:
                    categories[category] = {}
                if subcategory and subcategory not in categories[category]:
                    categories[category][subcategory] = []

                classified_documents[doc["doc_id"]] = {
                    "file_path": doc["file_path"],
                    "category": category,
                    "subcategory": subcategory,
                    "detail": detail,
                    "subdetail": subdetail,
                    "level": level  # ë¶„ë¥˜ ë ˆë²¨ ì •ë³´ ì €ì¥
                }

                # ë¡œê·¸ ì¶œë ¥ (ë ˆë²¨ì— ë§ê²Œ)
                log_parts = [category]
                if subcategory:
                    log_parts.append(subcategory)
                if detail:
                    log_parts.append(detail)
                if subdetail:
                    log_parts.append(subdetail)
                print(f"  âœ… ë¶„ë¥˜ ì™„ë£Œ: {' / '.join(log_parts)}")

            except Exception as e:
                print(f"  âš ï¸  ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©")
                classified_documents[doc["doc_id"]] = {
                    "file_path": doc["file_path"],
                    "category": "ì¼ë°˜ë¬¸ì„œ",
                    "subcategory": None,
                    "detail": None
                }
                if "ì¼ë°˜ë¬¸ì„œ" not in categories:
                    categories["ì¼ë°˜ë¬¸ì„œ"] = {}

        print(f"\nâœ… ëª¨ë“  ë¬¸ì„œ ë¶„ë¥˜ ì™„ë£Œ: {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬")

        # í´ë°± ì²˜ë¦¬ëŠ” ì´ë¯¸ ìœ„ì—ì„œ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬ë¨
        try:
            pass  # ì´ë¯¸ ì²˜ë¦¬ë¨

        except requests.exceptions.ConnectionError:
            print("âš ï¸  Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            # í´ë°±: ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ êµ¬ì¡°
            categories = {
                "ì¼ë°˜ë¬¸ì„œ": {}
            }
            classified_documents = {}
            for doc in documents:
                classified_documents[doc["doc_id"]] = {
                    "file_path": doc["file_path"],
                    "category": "ì¼ë°˜ë¬¸ì„œ",
                    "subcategory": None,
                    "detail": None
                }
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {e}. ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©")
            categories = {
                "ìë™ë¶„ë¥˜": {}
            }
            classified_documents = {}
            for doc in documents:
                classified_documents[doc["doc_id"]] = {
                    "file_path": doc["file_path"],
                    "category": "ìë™ë¶„ë¥˜",
                    "subcategory": None,
                    "detail": None
                }
        except Exception as e:
            print(f"âš ï¸  Gemma3 ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}. ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©")
            categories = {
                "ë¯¸ë¶„ë¥˜": {}
            }
            classified_documents = {}
            for doc in documents:
                classified_documents[doc["doc_id"]] = {
                    "file_path": doc["file_path"],
                    "category": "ë¯¸ë¶„ë¥˜",
                    "subcategory": None,
                    "detail": None
                }

        # DBì— ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ ë° ë³€ê²½ì´ë ¥ ê¸°ë¡
        for doc_id, classification in classified_documents.items():
            category = classification.get('category', 'Unknown')
            subcategory = classification.get('subcategory', '')
            detail = classification.get('detail', '')
            subdetail = classification.get('subdetail', '')
            file_path = classification.get('file_path', '')
            doc_level = classification.get('level', 1)

            # ë ˆë²¨ì— ë§ê²Œ agency/document_type ì„¤ì •
            if doc_level == 1:
                # 1ë‹¨ê³„: categoryë§Œ ì‚¬ìš©
                agency = category
                document_type = None  # 1ë‹¨ê³„ëŠ” document_type ì—†ìŒ
            elif doc_level == 2:
                # 2ë‹¨ê³„: category/subcategory
                agency = category
                document_type = subcategory if subcategory else None
            elif doc_level == 3:
                # 3ë‹¨ê³„: category/subcategory/detail
                agency = category
                if subcategory and detail:
                    document_type = f"{subcategory}/{detail}"
                elif subcategory:
                    document_type = subcategory
                else:
                    document_type = None
            else:
                # 4ë‹¨ê³„: category/subcategory/detail/subdetail
                agency = category
                parts = []
                if subcategory:
                    parts.append(subcategory)
                if detail:
                    parts.append(detail)
                if subdetail:
                    parts.append(subdetail)
                document_type = '/'.join(parts) if parts else None

            # ë¶„ë¥˜ ì •ë³´ ì»¬ëŸ¼ ì¶”ê°€ (ì—†ìœ¼ë©´)
            try:
                cur.execute("""
                    ALTER TABLE pdf_documents
                    ADD COLUMN IF NOT EXISTS agency VARCHAR(200),
                    ADD COLUMN IF NOT EXISTS document_type VARCHAR(200),
                    ADD COLUMN IF NOT EXISTS confidence_agency FLOAT,
                    ADD COLUMN IF NOT EXISTS confidence_document_type FLOAT,
                    ADD COLUMN IF NOT EXISTS is_classified BOOLEAN DEFAULT FALSE,
                    ADD COLUMN IF NOT EXISTS classified_date TIMESTAMP
                """)
            except:
                pass  # ì´ë¯¸ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ë¬´ì‹œ

            # ì´ì „ ë¶„ë¥˜ ì •ë³´ í™•ì¸ (UPDATE ì „ì—)
            cur.execute("""
                SELECT agency, document_type
                FROM pdf_documents
                WHERE doc_id = %s
            """, (doc_id,))
            prev_classification = cur.fetchone()

            change_type = 'created'
            prev_category = None
            if prev_classification and prev_classification[0]:
                change_type = 'updated'
                prev_category = f"{prev_classification[0]}/{prev_classification[1]}"

            # pdf_documents í…Œì´ë¸” ì—…ë°ì´íŠ¸
            cur.execute("""
                UPDATE pdf_documents
                SET status = 'CLASSIFIED',
                    agency = %s,
                    document_type = %s,
                    confidence_agency = %s,
                    confidence_document_type = %s,
                    is_classified = TRUE,
                    classified_date = NOW(),
                    updated_at = NOW()
                WHERE doc_id = %s
            """, (agency, document_type, 0.8, 0.8, doc_id))  # Gemma3ëŠ” ì‹ ë¢°ë„ 0.8ë¡œ ì„¤ì •

            # ë¡œê·¸ ì¶œë ¥ (ë ˆë²¨ì— ë§ê²Œ)
            save_log_parts = [agency]
            if document_type:
                save_log_parts.append(document_type)
            print(f"âœ… ë¬¸ì„œ ë¶„ë¥˜ ì €ì¥: doc_id={doc_id}, {' / '.join(save_log_parts)}")

            # ë³€ê²½ì´ë ¥ í…Œì´ë¸”ì—ë„ ê¸°ë¡
            try:
                # ì›ë³¸ ê²½ë¡œì—ì„œ ì‚¬ìš©ì í´ë”ëª… ì¶”ì¶œ
                file_name = file_path.split('/')[-1] if file_path else "Unknown"
                parts = file_path.split('/')
                top_folder = ""
                if len(parts) > 4 and parts[0] == '.':
                    folder_parts = parts[4:-1]
                    if folder_parts:
                        top_folder = folder_parts[0]

                # ë ˆë²¨ì— ë§ëŠ” ê²½ë¡œ ìƒì„±
                path_parts = [top_folder] if top_folder else []
                path_parts.append(agency)
                if document_type:
                    path_parts.append(document_type)
                path_parts.append(file_name)
                full_path_for_history = '/'.join(path_parts)

                # ë³€ê²½ì´ë ¥ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS classification_history (
                        history_id SERIAL PRIMARY KEY,
                        doc_id INTEGER NOT NULL,
                        file_name VARCHAR(500) NOT NULL,
                        full_path TEXT NOT NULL,
                        original_folder TEXT,
                        agency VARCHAR(200),
                        document_type VARCHAR(200),
                        confidence_agency FLOAT,
                        confidence_document_type FLOAT,
                        avg_confidence FLOAT,
                        change_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        change_type VARCHAR(50) NOT NULL,
                        previous_category VARCHAR(500)
                    )
                """)

                # original_folder ì»¬ëŸ¼ ì¶”ê°€ (ì—†ìœ¼ë©´)
                cur.execute("""
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_name='classification_history' AND column_name='original_folder'
                """)
                if not cur.fetchone():
                    cur.execute("ALTER TABLE classification_history ADD COLUMN original_folder TEXT")

                # ë³€ê²½ì´ë ¥ ê¸°ë¡
                avg_confidence = 0.8
                cur.execute("""
                    INSERT INTO classification_history
                    (doc_id, file_name, full_path, original_folder, agency, document_type,
                     confidence_agency, confidence_document_type, avg_confidence,
                     change_type, previous_category)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    doc_id, file_name, full_path_for_history, file_path,
                    agency, document_type, 0.8, 0.8, avg_confidence,
                    change_type, prev_category
                ))

                print(f"  ğŸ“ ë³€ê²½ì´ë ¥ ê¸°ë¡ ì™„ë£Œ (type={change_type})")

                # ì‘ì—… ë¡œê·¸ ê¸°ë¡
                classification_label = ' / '.join([p for p in [agency, document_type] if p])
                log_processing(
                    conn=conn,
                    doc_id=doc_id,
                    filename=file_path,
                    process_type='CLASSIFICATION',
                    status='SUCCESS',
                    message=f"Gemma3 ë¶„ë¥˜ ì™„ë£Œ: {file_name} - {classification_label}"
                )

            except Exception as history_error:
                print(f"  âš ï¸  ë³€ê²½ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œ): {history_error}")

        conn.commit()

        print(f"âœ… ì¹´í…Œê³ ë¦¬ ìƒì„± ë° ë¬¸ì„œ ë¶„ë¥˜ ì™„ë£Œ\n")
        print(f"{'='*60}\n")

        return {
            "success": True,
            "categories": categories,
            "classified_documents": classified_documents,
            "total_files": len(documents)
        }

    except Exception as e:
        print(f"âŒ ì¹´í…Œê³ ë¦¬ ìë™ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


# ============================================================================
# BERT í•™ìŠµ API (ìƒˆ ì¹´í…Œê³ ë¦¬ìš©)
# ============================================================================

@router.post("/category/train")
async def train_bert_model(request: Request):
    """
    ìƒ˜í”Œ ë¬¸ì„œë¡œ BERT ëª¨ë¸ í•™ìŠµ (ìƒˆ ì¹´í…Œê³ ë¦¬ ì‹œìŠ¤í…œìš©)

    Request Body:
        {
            "categories": {
                "category_name": ["sample_doc_id1", "sample_doc_id2", ...],
                ...
            }
        }

    Returns:
        {
            "success": bool,
            "model_path": str,  # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            "training_time": float
        }
    """
    try:
        data = await request.json()
        categories = data.get('categories', {})

        print(f"\n{'='*60}")
        print(f"ğŸ§  BERT ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print(f"   ì¹´í…Œê³ ë¦¬ ê°œìˆ˜: {len(categories)}")
        print(f"{'='*60}\n")

        if not categories:
            return {"success": False, "error": "ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

        conn = db_pool.get_conn()
        cur = conn.cursor()

        # 1. ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ ë¬¸ì„œ ìˆ˜ì§‘
        training_data = []
        for category, doc_ids in categories.items():
            print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ '{category}': {len(doc_ids)}ê°œ ìƒ˜í”Œ")

            for doc_id in doc_ids:
                # OCR í…ìŠ¤íŠ¸ ì¡°íšŒ
                cur.execute("""
                    SELECT full_text
                    FROM ocr_results
                    WHERE doc_id = %s
                    ORDER BY created_at DESC
                    LIMIT 1
                """, (doc_id,))

                row = cur.fetchone()
                if row and row[0]:
                    training_data.append({
                        "text": row[0],
                        "label": category
                    })

        if not training_data:
            return {"success": False, "error": "í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        print(f"\nğŸ“Š ì´ {len(training_data)}ê°œ ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ\n")

        # 2. BERT ëª¨ë¸ í•™ìŠµ
        print("ğŸ§  BERT ëª¨ë¸ í•™ìŠµ ì¤‘...")
        start_time = time.time()

        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
            from sklearn.model_selection import train_test_split
            import torch
            from torch.utils.data import Dataset

            # ë ˆì´ë¸” ì¸ì½”ë”©
            label_to_id = {label: idx for idx, label in enumerate(categories.keys())}
            id_to_label = {idx: label for label, idx in label_to_id.items()}

            # í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸” ë¶„ë¦¬
            texts = [item["text"] for item in training_data]
            labels = [label_to_id[item["label"]] for item in training_data]

            # í›ˆë ¨/ê²€ì¦ ë¶„í•  (80/20)
            train_texts, val_texts, train_labels, val_labels = train_test_split(
                texts, labels, test_size=0.2, random_state=42, stratify=labels if len(set(labels)) > 1 else None
            )

            # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì´ˆê¸°í™”
            model_name = "klue/bert-base"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                num_labels=len(categories),
                id2label=id_to_label,
                label2id=label_to_id
            )

            # í† í¬ë‚˜ì´ì§•
            train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
            val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

            # PyTorch Dataset ìƒì„±
            class CustomDataset(Dataset):
                def __init__(self, encodings, labels):
                    self.encodings = encodings
                    self.labels = labels

                def __getitem__(self, idx):
                    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
                    item['labels'] = torch.tensor(self.labels[idx])
                    return item

                def __len__(self):
                    return len(self.labels)

            train_dataset = CustomDataset(train_encodings, train_labels)
            val_dataset = CustomDataset(val_encodings, val_labels)

            # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            model_dir = f"models/bert_custom_{int(time.time())}"
            os.makedirs(model_dir, exist_ok=True)

            # í›ˆë ¨ ì„¤ì •
            training_args = TrainingArguments(
                output_dir=model_dir,
                num_train_epochs=3,
                per_device_train_batch_size=8,
                per_device_eval_batch_size=8,
                warmup_steps=100,
                weight_decay=0.01,
                logging_dir=f"{model_dir}/logs",
                logging_steps=10,
                eval_strategy="epoch",
                save_strategy="epoch",
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
                greater_is_better=False,
                save_total_limit=2,
                report_to="none"  # ì™¸ë¶€ ë¡œê¹… ë¹„í™œì„±í™”
            )

            # Trainer ì´ˆê¸°í™” ë° í•™ìŠµ
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset
            )

            print("ğŸš€ BERT ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘...")
            trainer.train()

            # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥
            model.save_pretrained(model_dir)
            tokenizer.save_pretrained(model_dir)

            # ë ˆì´ë¸” ë§¤í•‘ ì €ì¥
            label_mapping_path = os.path.join(model_dir, "label_mappings.json")
            with open(label_mapping_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "label2id": label_to_id,
                    "id2label": id_to_label,
                    "num_labels": len(categories)
                }, f, ensure_ascii=False, indent=2)

            # í•™ìŠµ ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = os.path.join(model_dir, "training_metadata.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "model_name": model_name,
                    "num_categories": len(categories),
                    "categories": list(categories.keys()),
                    "total_samples": len(training_data),
                    "train_samples": len(train_texts),
                    "val_samples": len(val_texts),
                    "training_date": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)

            training_time = time.time() - start_time

            print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ({training_time:.2f}ì´ˆ)")
            print(f"   ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_dir}")
            print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(train_texts)}ê°œ")
            print(f"   ê²€ì¦ ìƒ˜í”Œ: {len(val_texts)}ê°œ")
            print(f"{'='*60}\n")

            return {
                "success": True,
                "model_path": model_dir,
                "training_time": training_time,
                "total_samples": len(training_data),
                "train_samples": len(train_texts),
                "val_samples": len(val_texts),
                "categories": list(categories.keys()),
                "num_categories": len(categories)
            }

        except ImportError as e:
            print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("   pip install transformers torch scikit-learn ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”")
            return {
                "success": False,
                "error": f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {str(e)}",
                "message": "transformers, torch, scikit-learnì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”"
            }

        except Exception as e:
            print(f"âŒ BERT í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e),
                "message": "BERT ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨"
            }

    except Exception as e:
        print(f"âŒ BERT í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


# ============================================================================
# ì»¤ìŠ¤í…€ ëª¨ë¸ë¡œ ë¬¸ì„œ ë¶„ë¥˜ API
# ============================================================================

@router.post("/category/classify-with-custom-model")
async def classify_with_custom_model(request: Request):
    """
    ì»¤ìŠ¤í…€ í•™ìŠµëœ BERT ëª¨ë¸ë¡œ ë¬¸ì„œ ë¶„ë¥˜

    Request Body:
        {
            "model_path": "models/bert_custom_1234567890",
            "files": ["path1", "path2", ...]
        }

    Returns:
        {
            "success": bool,
            "results": [
                {"doc_id": 1, "file_path": "...", "category": "...", "confidence": 0.95},
                ...
            ]
        }
    """
    try:
        data = await request.json()
        model_path = data.get('model_path')
        files = data.get('files', [])

        print(f"\n{'='*60}")
        print(f"ğŸ” ì»¤ìŠ¤í…€ ëª¨ë¸ë¡œ ë¬¸ì„œ ë¶„ë¥˜ ì‹œì‘")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {model_path}")
        print(f"   íŒŒì¼ ê°œìˆ˜: {len(files)}")
        print(f"{'='*60}\n")

        if not model_path or not files:
            return {"success": False, "error": "model_pathì™€ filesê°€ í•„ìš”í•©ë‹ˆë‹¤"}

        if not os.path.exists(model_path):
            return {"success": False, "error": f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}"}

        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        import torch

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()

        # ë ˆì´ë¸” ë§¤í•‘ ë¡œë“œ
        label_mapping_path = os.path.join(model_path, "label_mappings.json")
        with open(label_mapping_path, 'r', encoding='utf-8') as f:
            label_mappings = json.load(f)

        id_to_label = {int(k): v for k, v in label_mappings['id2label'].items()}
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({len(id_to_label)}ê°œ ì¹´í…Œê³ ë¦¬)")

        conn = db_pool.get_conn()
        cur = conn.cursor()

        results = []

        # ê° íŒŒì¼ ë¶„ë¥˜
        for file_path in files:
            print(f"ğŸ“„ ë¶„ë¥˜ ì¤‘: {file_path}")

            # ê²½ë¡œ ì •ê·œí™”
            normalized_path = file_path.replace('\\', '/')
            if normalized_path.startswith('./'):
                normalized_path = normalized_path[2:]

            # doc_id ì¡°íšŒ
            cur.execute("""
                SELECT doc_id
                FROM pdf_documents
                WHERE filename = %s OR filename LIKE %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (normalized_path, f"%{normalized_path}"))

            row = cur.fetchone()
            if not row:
                print(f"âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                results.append({
                    "file_path": file_path,
                    "success": False,
                    "error": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                })
                continue

            doc_id = row[0]

            # OCR í…ìŠ¤íŠ¸ ì¡°íšŒ
            cur.execute("""
                SELECT full_text
                FROM ocr_results
                WHERE doc_id = %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (doc_id,))

            ocr_row = cur.fetchone()
            if not ocr_row or not ocr_row[0]:
                print(f"âš ï¸  OCR í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                results.append({
                    "doc_id": doc_id,
                    "file_path": file_path,
                    "success": False,
                    "error": "OCR í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                })
                continue

            full_text = ocr_row[0]

            # ë¶„ë¥˜ ìˆ˜í–‰
            inputs = tokenizer(
                full_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=-1)
                predicted_class = torch.argmax(probs, dim=-1).item()
                confidence = probs[0][predicted_class].item()

            predicted_category = id_to_label[predicted_class]

            print(f"âœ… ë¶„ë¥˜ ì™„ë£Œ: {predicted_category} (ì‹ ë¢°ë„: {confidence:.2%})")

            # DBì— ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
            cur.execute("""
                INSERT INTO document_keywords (doc_id, keywords, main_topic, keyword_count, raw_response, model_name)
                VALUES (%s, %s, %s, %s, %s, %s)
                RETURNING keyword_id
            """, (
                doc_id,
                json.dumps({"category": predicted_category, "confidence": confidence}, ensure_ascii=False),
                predicted_category,
                1,
                json.dumps({"all_probs": probs[0].cpu().tolist()}, ensure_ascii=False),
                f"custom-bert-{os.path.basename(model_path)}"
            ))

            keyword_id = cur.fetchone()[0]

            # ë¬¸ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸
            cur.execute("""
                UPDATE pdf_documents
                SET status = 'CLASSIFIED', updated_at = NOW()
                WHERE doc_id = %s
            """, (doc_id,))

            results.append({
                "doc_id": doc_id,
                "file_path": file_path,
                "category": predicted_category,
                "confidence": confidence,
                "keyword_id": keyword_id,
                "success": True
            })

        conn.commit()

        print(f"\nâœ… ì „ì²´ ë¶„ë¥˜ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ")
        print(f"{'='*60}\n")

        return {
            "success": True,
            "results": results,
            "total_files": len(files),
            "classified_files": len([r for r in results if r.get('success', False)])
        }

    except Exception as e:
        print(f"âŒ ì»¤ìŠ¤í…€ ëª¨ë¸ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


# ============================================================
# ë³€ê²½ì´ë ¥ API
# ============================================================

@router.post("/history/add")
async def add_classification_history(
    doc_id: int = Form(...),
    file_name: str = Form(...),
    full_path: str = Form(...),
    original_folder: str = Form(...),
    agency: str = Form(...),
    document_type: str = Form(...),
    confidence_agency: float = Form(...),
    confidence_document_type: float = Form(...),
    change_type: str = Form(...),  # 'created', 'updated', 'deleted'
    previous_category: Optional[str] = Form(None)
):
    """
    ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë³€ê²½ì´ë ¥ì— ì €ì¥
    """
    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # ë³€ê²½ì´ë ¥ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
        cur.execute("""
            CREATE TABLE IF NOT EXISTS classification_history (
                history_id SERIAL PRIMARY KEY,
                doc_id INTEGER NOT NULL,
                file_name VARCHAR(500) NOT NULL,
                full_path TEXT NOT NULL,
                original_folder TEXT,
                agency VARCHAR(200),
                document_type VARCHAR(200),
                confidence_agency FLOAT,
                confidence_document_type FLOAT,
                avg_confidence FLOAT,
                change_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                change_type VARCHAR(50) NOT NULL,
                previous_category VARCHAR(500)
            )
        """)

        # ê¸°ì¡´ í…Œì´ë¸”ì— original_folder ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
        cur.execute("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name='classification_history' AND column_name='original_folder'
        """)
        if not cur.fetchone():
            print("Adding 'original_folder' column to classification_history table...")
            cur.execute("""
                ALTER TABLE classification_history
                ADD COLUMN original_folder TEXT
            """)
            print("âœ… Column added successfully")
        conn.commit()

        # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
        avg_confidence = (confidence_agency + confidence_document_type) / 2

        print(f"ğŸ“Š Confidence ê°’ í™•ì¸:")
        print(f"  confidence_agency: {confidence_agency} (type: {type(confidence_agency)})")
        print(f"  confidence_document_type: {confidence_document_type} (type: {type(confidence_document_type)})")
        print(f"  avg_confidence: {avg_confidence}")

        # pdf_documentsì—ì„œ í˜„ì¬ ë¶„ë¥˜ ì •ë³´ í™•ì¸
        cur.execute("""
            SELECT agency, document_type, confidence_agency, confidence_document_type
            FROM pdf_documents
            WHERE doc_id = %s
        """, (doc_id,))
        current_classification = cur.fetchone()

        # ì´ì „ ë¶„ë¥˜ ì •ë³´ì™€ ë¹„êµ
        if current_classification:
            prev_agency, prev_document_type, prev_conf_agency, prev_conf_document = current_classification

            # ë¶„ë¥˜ê°€ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
            is_changed = (
                prev_agency != agency or
                prev_document_type != document_type
            )

            if prev_agency and prev_document_type:
                # ì´ë¯¸ ë¶„ë¥˜ë˜ì–´ ìˆì—ˆê³ , ë³€ê²½ì´ ìˆìœ¼ë©´ updated
                actual_change_type = 'updated' if is_changed else change_type
                prev_category = f"{prev_agency}/{prev_document_type}"
            else:
                # ì²˜ìŒ ë¶„ë¥˜ë˜ëŠ” ê²½ìš°
                actual_change_type = 'created'
                prev_category = previous_category
        else:
            # pdf_documentsì— ë ˆì½”ë“œê°€ ì—†ìœ¼ë©´ ì‹ ê·œ
            actual_change_type = 'created'
            prev_category = previous_category
            is_changed = True

        # ë³€ê²½ì´ ìˆì„ ë•Œë§Œ ì´ë ¥ì— ê¸°ë¡
        if is_changed or actual_change_type == 'deleted':
            cur.execute("""
                INSERT INTO classification_history
                (doc_id, file_name, full_path, original_folder, agency, document_type,
                 confidence_agency, confidence_document_type, avg_confidence,
                 change_type, previous_category)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING history_id, change_date
            """, (
                doc_id, file_name, full_path, original_folder, agency, document_type,
                confidence_agency, confidence_document_type, avg_confidence,
                actual_change_type, prev_category
            ))

            history_id, change_date = cur.fetchone()
            print(f"âœ… ë³€ê²½ì´ë ¥ ê¸°ë¡: history_id={history_id}, type={actual_change_type}, file={file_name}")
        else:
            # ë³€ê²½ì´ ì—†ìœ¼ë©´ ì´ë ¥ ê¸°ë¡í•˜ì§€ ì•ŠìŒ
            print(f"â„¹ï¸  ë³€ê²½ì‚¬í•­ ì—†ìŒ - ì´ë ¥ ê¸°ë¡ ìƒëµ: file={file_name}")
            history_id = None
            change_date = None

        conn.commit()

        print(f"âœ… ë¶„ë¥˜ ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ: file={file_name}")

        return {
            "success": True,
            "history_id": history_id,
            "change_date": change_date.isoformat() if change_date else None
        }

    except Exception as e:
        conn.rollback()
        print(f"âŒ ë³€ê²½ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


@router.get("/history/list")
async def get_classification_history(
    limit: int = 100,
    change_type: Optional[str] = None
):
    """
    ë³€ê²½ì´ë ¥ ì¡°íšŒ - pdf_documents í…Œì´ë¸” ê¸°ë°˜ (ì‹¤ì œ í˜„ì¬ ìƒíƒœ)
    """
    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # pdf_documentsì—ì„œ ë¶„ë¥˜ëœ íŒŒì¼ë“¤ ì¡°íšŒ (ì‹¤ì œ í˜„ì¬ ìƒíƒœ)
        where_clause = "WHERE is_classified = TRUE"
        params = []

        # change_type í•„í„°ëŠ” classification_historyë¥¼ ì°¸ì¡°
        # í•˜ì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œëŠ” pdf_documentsì˜ í˜„ì¬ ìƒíƒœë¥¼ ë³´ì—¬ì¤Œ

        query = f"""
            SELECT
                p.doc_id,
                p.filename,
                p.filename as full_path,
                p.filename as original_folder,
                p.agency,
                p.document_type,
                p.confidence_agency,
                p.confidence_document_type,
                (p.confidence_agency + p.confidence_document_type) / 2.0 as avg_confidence,
                p.classified_date,
                'created' as change_type
            FROM pdf_documents p
            {where_clause}
            ORDER BY p.classified_date DESC
            LIMIT %s
        """
        params.append(limit)

        cur.execute(query, tuple(params))
        rows = cur.fetchall()

        history = []
        for row in rows:
            doc_id = row[0]
            filename = row[1]
            agency = row[4]
            document_type = row[5]
            classified_date = row[9]

            # íŒŒì¼ëª… ì¶”ì¶œ
            file_name = filename.split('/')[-1] if filename else "Unknown"

            # ì›ë³¸ ê²½ë¡œì—ì„œ ì‚¬ìš©ì í´ë”ëª… ì¶”ì¶œ (./upload/username/uid/í´ë”ëª…/... í˜•íƒœì—ì„œ í´ë”ëª… ì¶”ì¶œ)
            top_folder = ""
            if filename:
                # ./upload/username/uid/ ì´í›„ì˜ ê²½ë¡œ ì¶”ì¶œ
                parts = filename.split('/')
                # ./upload/username/uid/í´ë”ëª…/íŒŒì¼.pdf í˜•íƒœì—ì„œ í´ë”ëª…ì€ ì¸ë±ìŠ¤ 4
                if len(parts) > 4 and parts[0] == '.':
                    # upload ë‹¤ìŒ ì‚¬ìš©ì ì •ë³´ë¥¼ ê±´ë„ˆë›°ê³  ì‹¤ì œ í´ë”ëª… ì¶”ì¶œ
                    folder_parts = parts[4:-1]  # ë§ˆì§€ë§‰ì€ íŒŒì¼ëª…ì´ë¯€ë¡œ ì œì™¸
                    if folder_parts:
                        top_folder = folder_parts[0]  # ì²« ë²ˆì§¸ í´ë”ëª…
                elif len(parts) > 1:
                    # ë‹¤ë¥¸ í˜•íƒœì˜ ê²½ë¡œë©´ ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” í´ë”ëª… ì‚¬ìš©
                    for part in parts[:-1]:  # ë§ˆì§€ë§‰ íŒŒì¼ëª… ì œì™¸
                        if part and part not in ['.', 'upload']:
                            top_folder = part
                            break

            # ë¶„ë¥˜ëœ ì „ì²´ ê²½ë¡œ ìƒì„± (ë ˆë²¨ì— ë§ê²Œ)
            # ë ˆë²¨1: ìµœìƒìœ„í´ë”/ê¸°ê´€/íŒŒì¼ëª…
            # ë ˆë²¨2+: ìµœìƒìœ„í´ë”/ê¸°ê´€/ë¬¸ì„œìœ í˜•/íŒŒì¼ëª…
            path_parts = []
            if top_folder:
                path_parts.append(top_folder)
            path_parts.append(agency)
            if document_type:
                path_parts.append(document_type)
            path_parts.append(file_name)
            full_path = '/'.join(path_parts)

            history.append({
                "id": str(doc_id),
                "doc_id": doc_id,
                "fileName": file_name,
                "fullPath": full_path,
                "originalFolder": filename,  # ì›ë³¸ ê²½ë¡œ
                "agency": agency,
                "documentType": document_type if document_type else "",
                "confidenceAgency": row[6],
                "confidenceDocumentType": row[7],
                "confidence": int(row[8] * 100) if row[8] else 0,  # í‰ê·  ì‹ ë¢°ë„ (0~1 â†’ 0~100)
                "changeDate": classified_date.strftime("%Y-%m-%d %H:%M:%S") if classified_date else "",
                "changeType": row[10],
                "previousCategory": None
            })

        return {
            "success": True,
            "history": history,
            "total": len(history)
        }

    except Exception as e:
        print(f"âŒ ë³€ê²½ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


# ============================================================
# í†µê³„ API
# ============================================================

@router.get("/statistics/overall")
async def get_overall_statistics():
    """
    ì „ì²´ íŒŒì¼ ë¶„ë¥˜ í†µê³„ ì¡°íšŒ
    - ì „ì²´ íŒŒì¼ ìˆ˜
    - ë¶„ë¥˜ ì™„ë£Œ íŒŒì¼ ìˆ˜
    - ë¯¸ë¶„ë¥˜ íŒŒì¼ ìˆ˜
    - ì¹´í…Œê³ ë¦¬ë³„(agency) íŒŒì¼ ë¶„í¬
    - ë¬¸ì„œìœ í˜•ë³„(document_type) íŒŒì¼ ë¶„í¬
    """
    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # 1. ì „ì²´ íŒŒì¼ ìˆ˜
        cur.execute("SELECT COUNT(*) FROM pdf_documents")
        total_files = cur.fetchone()[0]

        # 2. ë¶„ë¥˜ ì™„ë£Œ íŒŒì¼ ìˆ˜
        cur.execute("SELECT COUNT(*) FROM pdf_documents WHERE is_classified = TRUE")
        classified_files = cur.fetchone()[0]

        # 3. ë¯¸ë¶„ë¥˜ íŒŒì¼ ìˆ˜
        unclassified_files = total_files - classified_files

        # 4. ê¸°ê´€ë³„(agency) íŒŒì¼ ë¶„í¬
        cur.execute("""
            SELECT agency, COUNT(*) as count
            FROM pdf_documents
            WHERE is_classified = TRUE AND agency IS NOT NULL
            GROUP BY agency
            ORDER BY count DESC
        """)
        agency_distribution = []
        for row in cur.fetchall():
            agency_distribution.append({
                "name": row[0],
                "count": row[1]
            })

        # 5. ë¬¸ì„œìœ í˜•ë³„(document_type) íŒŒì¼ ë¶„í¬ - ìƒìœ„ ì¹´í…Œê³ ë¦¬ë§Œ
        cur.execute("""
            SELECT document_type, COUNT(*) as count
            FROM pdf_documents
            WHERE is_classified = TRUE AND document_type IS NOT NULL
            GROUP BY document_type
            ORDER BY count DESC
            LIMIT 10
        """)
        document_type_distribution = []
        for row in cur.fetchall():
            # document_typeì´ "subcategory/detail" í˜•íƒœë©´ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©
            doc_type = row[0]
            if doc_type:
                main_type = doc_type.split('/')[0]
                document_type_distribution.append({
                    "name": main_type,
                    "count": row[1]
                })

        # ë¯¸ë¶„ë¥˜ë„ ì¶”ê°€
        if unclassified_files > 0:
            document_type_distribution.append({
                "name": "ë¯¸ë¶„ë¥˜",
                "count": unclassified_files
            })

        return {
            "success": True,
            "totalFiles": total_files,
            "classifiedFiles": classified_files,
            "unclassifiedFiles": unclassified_files,
            "classificationRate": round((classified_files / total_files * 100), 1) if total_files > 0 else 0,
            "agencyDistribution": agency_distribution,
            "documentTypeDistribution": document_type_distribution
        }

    except Exception as e:
        print(f"âŒ ì „ì²´ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)


@router.get("/statistics/folders")
async def get_folder_statistics():
    """
    í´ë”ë³„ íŒŒì¼ ë¶„ë¥˜ í†µê³„ ì¡°íšŒ
    - ê° í´ë”ì˜ ì „ì²´ íŒŒì¼ ìˆ˜
    - ë¶„ë¥˜ ì™„ë£Œ/ë¯¸ë¶„ë¥˜ íŒŒì¼ ìˆ˜
    - ë¶„ë¥˜ìœ¨
    - í´ë”ë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬
    """
    conn = db_pool.get_conn()
    cur = conn.cursor()

    try:
        # ì „ì²´ íŒŒì¼ ì¡°íšŒ (í´ë”ëª… í¬í•¨)
        cur.execute("""
            SELECT
                filename,
                is_classified,
                agency,
                document_type
            FROM pdf_documents
            ORDER BY filename
        """)

        files = cur.fetchall()

        # í´ë”ë³„ë¡œ ê·¸ë£¹í™”
        folder_stats = {}

        for row in files:
            filename = row[0]
            is_classified = row[1]
            agency = row[2]
            document_type = row[3]

            # í´ë”ëª… ì¶”ì¶œ (./upload/username/uid/í´ë”ëª…/íŒŒì¼.pdf í˜•íƒœ)
            folder_name = "ê¸°íƒ€"
            if filename:
                parts = filename.split('/')
                if len(parts) > 4 and parts[0] == '.':
                    folder_parts = parts[4:-1]
                    if folder_parts:
                        folder_name = folder_parts[0]
                elif len(parts) > 1:
                    for part in parts[:-1]:
                        if part and part not in ['.', 'upload']:
                            folder_name = part
                            break

            # í´ë” í†µê³„ ì´ˆê¸°í™”
            if folder_name not in folder_stats:
                folder_stats[folder_name] = {
                    "name": folder_name,
                    "totalFiles": 0,
                    "classifiedFiles": 0,
                    "unclassifiedFiles": 0,
                    "categories": {}
                }

            # íŒŒì¼ ìˆ˜ ì¹´ìš´íŠ¸
            folder_stats[folder_name]["totalFiles"] += 1

            if is_classified:
                folder_stats[folder_name]["classifiedFiles"] += 1

                # ì¹´í…Œê³ ë¦¬ ì¹´ìš´íŠ¸ (document_typeì˜ ì²« ë¶€ë¶„ë§Œ ì‚¬ìš©)
                category = "ê¸°íƒ€"
                if document_type:
                    category = document_type.split('/')[0]
                elif agency:
                    category = agency

                if category not in folder_stats[folder_name]["categories"]:
                    folder_stats[folder_name]["categories"][category] = 0
                folder_stats[folder_name]["categories"][category] += 1
            else:
                folder_stats[folder_name]["unclassifiedFiles"] += 1
                if "ë¯¸ë¶„ë¥˜" not in folder_stats[folder_name]["categories"]:
                    folder_stats[folder_name]["categories"]["ë¯¸ë¶„ë¥˜"] = 0
                folder_stats[folder_name]["categories"]["ë¯¸ë¶„ë¥˜"] += 1

        # ê²°ê³¼ í¬ë§·íŒ…
        result = []
        for folder_name, stats in folder_stats.items():
            total = stats["totalFiles"]
            classified = stats["classifiedFiles"]
            classification_rate = round((classified / total * 100), 1) if total > 0 else 0

            # ì¹´í…Œê³ ë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            categories = []
            for cat_name, count in stats["categories"].items():
                percentage = round((count / total * 100), 1) if total > 0 else 0
                categories.append({
                    "name": cat_name,
                    "count": count,
                    "percentage": percentage
                })

            # ì¹´ìš´íŠ¸ ìˆœìœ¼ë¡œ ì •ë ¬
            categories.sort(key=lambda x: x["count"], reverse=True)

            result.append({
                "name": folder_name,
                "totalFiles": total,
                "classifiedFiles": classified,
                "unclassifiedFiles": stats["unclassifiedFiles"],
                "classificationRate": classification_rate,
                "categories": categories
            })

        # ì „ì²´ íŒŒì¼ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        result.sort(key=lambda x: x["totalFiles"], reverse=True)

        return {
            "success": True,
            "folders": result,
            "total": len(result)
        }

    except Exception as e:
        print(f"âŒ í´ë”ë³„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
    finally:
        cur.close()
        db_pool.release_conn(conn)

